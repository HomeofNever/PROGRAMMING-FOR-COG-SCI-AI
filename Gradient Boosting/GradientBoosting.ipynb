{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_squared_error,confusion_matrix,accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* History\n",
    "    - Leo Breiman (1998,1999) formulated ADABoost as gradient descent with special loss function\n",
    "    - Friedman (2000,2001) formulated ADABoost with generalized  loss function\n",
    "* Formulates the boosting problem as an optimization problem using a Loss function\n",
    "    - Gradually reduce the loss by adding more weak learners\n",
    "* For regression, fit each weak learner to the residuals of the previous weak learner\n",
    "* Can be used for regression and classification\n",
    "    - Classification is more difficult\n",
    "\n",
    "### Compare with ADABoost\n",
    "\n",
    "#### Similarities\n",
    "\n",
    "* Like ADABoost we iteratively add weak learners to build a stronger learner to improve performance\n",
    "\n",
    "#### Differences\n",
    "\n",
    "* Gradient Boosting identifies errors by residuals; ADABoost identifies errors by high weight values, \n",
    "* Gradient Boosting scales trees by constant amount, the learning rate, versus voting power $\\alpha$s in ADABoost\n",
    "    - Although sklearn does have a learning rate parameter for AdaBoost??\n",
    "* Gradient Boosting grows more complex trees (i.e. trees with 8 to 32 leaves), ADABoost almost always uses stumps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Ideas\n",
    "\n",
    "Given: $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$\n",
    "\n",
    "Problem: Fit a model F(x) to minimize square loss\n",
    "\n",
    "Think of a tree as a function, inputs a new value and outputs a prediction for that input (derived from the leaf node)\n",
    "\n",
    "Initial model F(x) is good but not perfect, for example, its Mean Squared Error is 20.\n",
    "\n",
    "\n",
    "You can add a model h(x) to F(x): so the new prediction will be F(x) + h(x)\n",
    "\n",
    "Ideally we would like h(x) to satisfy:\n",
    "\n",
    "$$F(x_1) + h(x_1) = y_1$$\n",
    "$$F(x_2) + h(x_2) = y_2$$\n",
    "$$\\vdots$$\n",
    "$$F(x_n) + h(x_n) = y_n$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$h(x_1) = y_1 - F(x_1) $$\n",
    "$$h(x_2) = y_2 - F(x_2) $$\n",
    "$$\\vdots$$\n",
    "$$h(x_n) = y_n - F(x_n) $$\n",
    "\n",
    "$y_i - F(x_i)$ are the residuals.\n",
    "\n",
    "By fitting h(x) to the residuals we will get a better model since the residuals indicate which data points are problematic.\n",
    "\n",
    "\n",
    "Fit h(x) to:\n",
    "$$ (x_1,y_1 - F(x_1)),(x_2,y_2 - F(x_2)),...,(x_n,y_1 - F(x_n))$$\n",
    "\n",
    "We can repeat this process to keep improving the fit, so \n",
    "\n",
    "$$ F(x) + \\gamma h_1(x) + \\gamma h_2(x) + ... + \\gamma h_m(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gradient in Gradient Boosting\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "Minimize a loss function L by moving in the opposite direction of the gradient.\n",
    "\n",
    "$$ \\theta_{t + 1} = \\theta_t - \\gamma \\frac{\\partial{L}}{\\partial{\\theta_t}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,100)\n",
    "y = x**2\n",
    "plt.plot(x,y)\n",
    "dy = 2*x\n",
    "x_1 = 1\n",
    "y_1 = x_1**2\n",
    "plt.plot(x_1,y_1,'r>',MarkerSize = 10)\n",
    "z = np.linspace(0,2)\n",
    "y_line = 2*(z - x_1) + y_1\n",
    "plt.plot(z,y_line,'b-')\n",
    "plt.title(\"Positive slope, move down the curve  \");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function: $L(y, F(x)) = \\frac{1}{2}(y - F(x))^2$\n",
    "    (Note: this 1/2 times OLS Loss Function (i.e. RSS))\n",
    "\n",
    "Minimize $ J = \\sum_i{L(y_i, F(x_i))}$ by adjusting $F(x_1),F(x_2), \\ldots, F(x_n)$\n",
    "\n",
    "Take derivatives: \n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{F(x_i)}} = \\frac{\\partial{\\sum_i L(y_i,F(x_i))}}{\\partial{F(x_i)}} = \\frac{\\partial{L(y_i,F(x_i))}}{\\partial{F(x_i)}} = -(y_i - F(x_i)) = F(x_i) - y_i$$\n",
    "\n",
    "Therefore the residuals are the negative gradients:\n",
    "\n",
    "$$ y_i - F(x_i) = - \\frac{\\partial{J}}{\\partial{F(x_i)}}$$\n",
    "\n",
    "Updating the function is like doing Gradient Descent:\n",
    "\n",
    "$$ F_{t+1}(x_i) = F_t(x_i) + h(x_i)$$\n",
    "$$ F_{t+1}(x_i) = F_t(x_i) + y_i - F_t(x_i)$$\n",
    "$$ F_{t+1}(x_i) = F_t(x_i) - 1\\frac{\\partial{J}}{\\partial{F(x_i)}}$$\n",
    "\n",
    "For regression with square loss: $residual \\Longleftrightarrow negative \\text{ }gradient$\n",
    "\n",
    "Updating by fitting the residual is updating by Gradient Descent\n",
    "\n",
    "#### Updating by Gradient Descent generalizes to other loss functions\n",
    "\n",
    "* Square error is not robust to outliers (because of magnifying the error by squaring it)\n",
    "\n",
    "* Absolute loss L(y,F(x)) = |y - F(x)| is more robust to outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y - the target\n",
    "# yhat - the fitted values\n",
    "\n",
    "def calc_loss(y,yhat):\n",
    "    return np.mean(1/2*(y - yhat)**2)\n",
    "\n",
    "def gradient(y,yhat):\n",
    "    return y - yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Years = [5,7,12,23,25,28,29,34,35,40]\n",
    "Salary = [82,80,103,118,172,127,204,189,99,166]\n",
    "df = pd.DataFrame({'Years':Years,'Salary': Salary})\n",
    "X = df.loc[:,['Years']].values\n",
    "y = df.loc[:,'Salary'].values\n",
    "plt.plot(X,y,'o')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Salary');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth = 3,random_state=42)\n",
    "yhat = np.mean(y)\n",
    "loss = []\n",
    "alpha = .1\n",
    "u = gradient(y,yhat)\n",
    "M = 100\n",
    "for i in range(M):\n",
    "    model.fit(X,u)\n",
    "    yhat = yhat + alpha*model.predict(X)\n",
    "    u = gradient(y,yhat)\n",
    "    loss.append(calc_loss(y,yhat))\n",
    "    if i%20 == 0: print(f'Iteration {i}, loss, {loss[-1]}')\n",
    "    if loss[-1] < .05: \n",
    "        print(f'Number of Iterations {i}')\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'Initial loss {np.round(loss[0],2)}, Final Loss {np.round(loss[-1],2)}')\n",
    "print(f'Min residual {np.round(np.min(u),2)}, Max residual {np.round(np.max(u),2)}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.plot(X,yhat,'o')\n",
    "ax1.set_ylabel('yhat')\n",
    "ax1.set_xlabel('Years')\n",
    "ax2.plot(X,u,'o')\n",
    "ax2.set_ylim(-10,10)\n",
    "ax2.set_ylabel('residual')\n",
    "ax2.set_xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(i+1),loss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Algorithm for regression trees\n",
    "\n",
    "* Fit each model to the residuals of the previous model\n",
    " \n",
    "#### Input: $(x_i,y_i)$ for i = 1,2,..,n and a Loss function $L(y_i,F(x))$\n",
    "* Loss function: $L(y, F(x)) = \\frac{1}{2}(y - F(x))^2$\n",
    "* F(x) is the predicted value, i.e. output of the tree function\n",
    "\n",
    "#### 1. Initialize model with\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$F_0(x) = \\underset{\\gamma}{\\mathrm{argmin}} \\sum_i^nL(y_i,\\gamma)$$  \n",
    "</div>\n",
    "\n",
    "* Initial prediction: average of $y_i$s (the observed values)\n",
    "\n",
    "#### 2. For m = 1,...,M, i = 1,..,n\n",
    "* m = tree number, (e.g. 100)\n",
    "* i = observation number\n",
    "* Sequentially add trees\n",
    "\n",
    "#### 2a. Compute psuedo-residuals\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$r_{im} = - \\frac{\\partial L(y_i,F(x_i))}{\\partial F(x_i)}$$ \n",
    "</div>\n",
    "\n",
    "* This is just the residual: (observed - predicted) where observed = $y_i$ and predicted = $F_{m-1}(x)$ from last round\n",
    "* $r_{im}$ is called a pseudo-residual\n",
    "* Pseudo because of the 1/2\n",
    "\n",
    "#### 2b. Fit a regression tree to the residuals $r_{im}$s\n",
    "\n",
    "* The leaf nodes of the tree are regions $R_{jm}$, j = 1,..,$J_m$, the number of leaf nodes\n",
    "\n",
    "#### 2c. Calculate output values of tree\n",
    "\n",
    "**For j = 1,...,$J_m$**\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\gamma_{jm} = \\underset{\\gamma}{\\mathrm{argmin}} \\sum_{x_i \\in R_{ij}}L(y_i,F_{m-1}(x_i) + \\gamma)$$\n",
    "    \n",
    "* $\\gamma_{jm}$ is the output of leaf node j of tree m\n",
    "* For this Loss function it is just the average of the leaf node values\n",
    "\n",
    "#### 2d. Update Classifier, make new prediction\n",
    "\n",
    "* Add output of tree m\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$F_m(x) = F_{m - 1}(x) + \\nu\\sum_{j=1}^{J_m}\\gamma_{jm}I(x \\in R_{jm})$$\n",
    "</div>\n",
    "\n",
    "* $\\nu$ is learning rate\n",
    "* The summation is over data points in the region\n",
    "\n",
    "#### 3. New data value\n",
    "\n",
    "* Compute $y_i$ of new data point using $F_M(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow learning \n",
    "\n",
    "* Fitting small trees to the residuals slowly improves $F(x)$ in areas where it does not perform well.\n",
    "* The shrinkage parameter $\\nu$ slows the process down even further.\n",
    "* In general, statistical learning methods that learn slowly tend to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting in sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "\n",
    "#### Gradient Boosting Tuning Parameters\n",
    "\n",
    "* Loss: (default 'ls'- least squares)\n",
    "    - Loss function L\n",
    "* n_estimators (default 100) M\n",
    "    - The number of trees   \n",
    "* learning_rate (default 0.1) $\\nu$: \n",
    "    - Shrinkage parameter controls rate of learning\n",
    "* max_depth: \n",
    "    - Control size by limiting tree depth\n",
    "* min_samples_split: \n",
    "    - Minimum # of samples required to split an internal node\n",
    "* min_samples_leaf: \n",
    "    - Control size by setting minimum number of samples at leaf nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motorcycle data\n",
    "\n",
    "https://www.stat.cmu.edu/~larry/all-of-statistics/=data/motor.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = pd.read_csv(\"Motorcycle.csv\")\n",
    "plt.plot(mc.times,mc.accel,'o')\n",
    "plt.xlabel('times')\n",
    "plt.ylabel('accel')\n",
    "mc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mc.loc[:,['times']].values\n",
    "y = mc.loc[:,'accel'].values\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(max_depth = 2, \n",
    "                                  criterion = 'mse',\n",
    "                                  n_estimators = 100,\n",
    "                                  learning_rate = .1,\n",
    "                                  subsample = 1,\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  random_state=42)\n",
    "model.fit(X,y)\n",
    "yhat = model.predict(X)\n",
    "\n",
    "print('Features: ',model.feature_importances_)\n",
    "print('R-squared Score: ',np.round(model.score(X,y),3))\n",
    "print('MSE: ',np.round(np.mean((y - yhat)**2),2))\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.plot(X,y,'o')\n",
    "ax1.plot(X,yhat,'r')\n",
    "ax2.plot(X,gradient(y,yhat),'o')\n",
    "ax2.set_ylim(-10,10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boston Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv('Boston.csv')\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = boston.columns.tolist()[0:-1] ## ['crim','rm','age'] ##\n",
    "print(feats)\n",
    "X = boston.loc[:, feats].values\n",
    "y = boston.loc[:, 'medv'].values\n",
    "\n",
    "# Make Validation Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1234)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to traing data\n",
    "m = GradientBoostingRegressor(n_estimators = 500, max_depth =6, learning_rate = 0.1)\n",
    "m.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "yhat = m.predict(X_test)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, yhat)\n",
    "mse = mean_squared_error(y_test,yhat)\n",
    "print(\"R2: %.4f\" % r2)\n",
    "print('MSE',np.round(mse,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test set deviance\n",
    "test_score = np.zeros((500,), dtype=np.float64)\n",
    "\n",
    "for i, yhat in enumerate(m.staged_predict(X_test)):\n",
    "    test_score[i] = m.loss_(y_test, yhat)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('Loss')\n",
    "plt.plot(np.arange(500) + 1, m.train_score_, 'b-',label='Training Set Loss')\n",
    "plt.plot(np.arange(500) + 1, test_score, 'r-',label='Test Set Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = m.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "feat_names = feats \n",
    "sorted_names = [feat_names[i] for i in sorted_idx]\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos,sorted_names)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(loss = 'ls', random_state = 42)\n",
    "parameters = [{'max_depth': [2,4,6,8],\n",
    "               'n_estimators': [100,200,500],\n",
    "               'learning_rate': [.01,.1,.5],\n",
    "               'min_samples_leaf': [1,3],\n",
    "               'subsample': [.5,1]\n",
    "               }]\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'r2', #'neg_mean_squared_error',\n",
    "                           cv = 5,\n",
    "                           iid = 'False',\n",
    "                           verbose = 1,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X, y)\n",
    "print(\"Best accuracy: \", grid_search.best_score_)\n",
    "print(\"Best parameters: \", grid_search.best_params_ )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classification\n",
    "\n",
    "* Same basic algorithm used for regression\n",
    "* Related to Logistic Regression using the logit (i.e. log odds) and logistic function.\n",
    "* Predict log(odds) rather than residuals\n",
    "    - Remember odds = $\\frac{p}{1-p}$, p = probability\n",
    "* Loss function\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ L(y,F(x)) = -ylog(odds) + log(1+e^{log(odds)})$$\n",
    "</div>\n",
    "\n",
    "* Derivative of Loss Function\n",
    "    \n",
    "<div style=\"font-size: 115%;\">    \n",
    "$$\\frac{\\partial L}{F(x)} = -y + logistic(log(odds)) = -y + p$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classification in slkearn\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "#### Gradient Boosting Tuning Parameters\n",
    "\n",
    "* Loss: (default 'deviance')\n",
    "    - Logistic Regression for probabilistic outputs\n",
    "* n_estimators (default 100) M\n",
    "    - The number of trees\n",
    "* learning_rate (default 0.1) $\\nu$: \n",
    "    - Shrinkage parameter controls rate of learning\n",
    "* max_depth: \n",
    "    - Control size by limiting tree depth\n",
    "* min_samples_split: \n",
    "    - Minimum # of samples required to split an internal node\n",
    "* min_samples_leaf: \n",
    "    - Control size by setting minimum number of samples at leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='times',y='accel',hue='strata',data=mc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mc.loc[:,['times','accel']]\n",
    "y = mc.loc[:,'strata']\n",
    "# Make Validation Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  stratify = y,test_size = 0.25, random_state = 1234)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test,yhat)\n",
    "print(cm)\n",
    "accuracy_score(y_test,yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**Gradient Boosting Algorithm:** STAT Quest, Gradient Boosting Parts 1 - 4. https://www.youtube.com/watch?v=3CC4N4z3GJc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
