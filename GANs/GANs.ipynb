{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANS)\n",
    "\n",
    "* Generative Adversarial Nets(2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair Aaron Courville, and Yoshua Bengio.   \n",
    "https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf  \n",
    "https://www.youtube.com/watch?v=eyxmSmjmNS0\n",
    "\n",
    "*  A generative model with two parts. The goal is to produce realistic images by having the **Generator** create an image that the **Discriminator** classifies as either real or fake.\n",
    "\n",
    "* Generative models learn P(x,y), Discrimative models learn P(y|x)\n",
    "    - We can sample from Generative Models\n",
    "\n",
    "* A zero-sum game between the Generator and the Discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](GAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "\n",
    "* It takes a random noise from some probability distribution as input and tries to generate a realistic output image\n",
    "\n",
    "* $G(z,\\theta_g), z \\sim{N(0,1)} \\text{ or } z \\sim{U(-1,1)}$ Sample from a Normal or Uniform distribution\n",
    "\n",
    "\n",
    "#### Discriminator\n",
    "\n",
    "* It takes two alternating inputs: the real images of the training dataset or the generated fake samples from the generator. \n",
    "* It classifies the input image as real or fake (i.e. comes from the Generator)\n",
    "\n",
    "* $D(x,\\theta_d)$  \n",
    "* Input: $z \\sim{p_g(z)}$ or $x \\sim{p_{data}(x)}$  \n",
    "* Output: 1 = real, 0 = fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "#### Disciminator Loss Function\n",
    "\n",
    "* For the Disciminator we want to minimize the loss function.\n",
    "\n",
    "$$J^{(D)} = \\mathbb{E}_{x\\sim{p_{data}}}log(D(x)) + \\mathbb{E}_{z\\sim{p_z}}log(1 - D(G(z))$$\n",
    "\n",
    "\n",
    "* $\\mathbb{E}_{x\\sim{p_{data}}}log(D(x))$ is the loss when input is sampled from the real data. \n",
    "\n",
    "* $\\mathbb{E}_{z\\sim{p_z}}log(1 - D(G(z))$ is the loss when the input is sampled from the Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Generator Loss function\n",
    "\n",
    "$$J^{(G)} = \\mathbb{E}_{z\\sim{p_z}}log(1 - D(G(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the Generator and Discriminator Loss Function\n",
    "\n",
    "$$\\underset{G}{\\mathrm{min}}\\text{ }\\underset{D}{\\mathrm{max}}V(G,D) = \\mathbb{E}_{x\\sim{p_{data}}}log(D(x)) + \\mathbb{E}_{z\\sim{p_z}}log(1 - D(G(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "* These networks are hard to train\n",
    "\n",
    "* The generator and the discriminator are trained separately. \n",
    "\n",
    "* They are trained sequentially (i.e. one after the other), and alternate between the two over multiple epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "* 1. Repeat for k steps, where k is a hyperparameter (set k = 1):  \n",
    "    - Sample a mini-batch of m noise samples $(z^{(1)},z^{(2)},...,z^{(m)})$ and transform with the Generator\n",
    "    - Sample a mini-batch of m samples from the real data, $(x^{(1)},x^{(2)},...,x^{(m)})$\n",
    "    - Update the discriminator weights $\\theta_d$ by **ascending** the stochastic gradient of its loss:\n",
    "$$\\nabla_{\\theta_d}\\frac{1}{m}\\sum_i^m[log(D(x^{(i)})) + log(1 - D(G(z^{(i)}))]$$\n",
    "    - The generator weights $\\theta_g$ will be locked and only the discriminator weights $\\theta_d$ are updated.\n",
    "    \n",
    "* 2. Sample a mini-batch of m noise samples $(z^{(1)},z^{(2)},...,z^{(m)})$ and transform with the Generator\n",
    "* 3.  Update the generator by **descending** the stochastic gradient of its loss:\n",
    "$$\\nabla_{\\theta_d}\\frac{1}{m}\\sum_i^m[ log(1 - D(G(z^{(i)}))]$$\n",
    "    - The discriminator weights $\\theta_d$  are locked and we can only adjust the Generator weights $\\theta_g$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Tricks \n",
    "\n",
    "\n",
    "* Training GANs is notoriously difficult, below are a few of the tricks (i.e. heuristics) to try\n",
    "\n",
    "* Use tanh as the last activation in the generator, instead of the sigmoid\n",
    "* Sample points from the latent space using a Gaussian not a uniform distribution.\n",
    "* Introduce randomness ways: \n",
    "    - Use dropout in the discriminator, \n",
    "    - Add some random noise to the labels for the discriminator.\n",
    "* Use LeakyReLU instead of a ReLU activation to ease sparsity constraints by allowing small negative activation values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vasilev,Slatr,Spacagna,Roelants,Zocca (2019) Python Deep Learning, 2nd Edition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
