{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA,KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "* **Unsupervised** dimensionality reduction method\n",
    " \n",
    "* PCA produces a low-dimensional representation of a dataset that contains as much as possible of the variation in the dataset.\n",
    "    - The dataset has n rows (observations) and p columns (features, predictors)\n",
    "    - Want to reduce the number of features by combining them to a lower dimension (d) of the feature space\n",
    "        - Some features may be redundant, others may not have much influence on the response variable\n",
    " \n",
    "* PCA finds d linear combinations of the p features, that:  \n",
    "    - Have maximal variance\n",
    "    - Are mutually uncorrelated.\n",
    "* The d linear combinations form a new representation of the data in a lower dimension\n",
    " \n",
    "* These derived variables produced by PCA are called the Principal Components(PCs) and can be used in supervised learning problems\n",
    "    - Principal Component regression uses the PCs as predictors\n",
    "    - How to interpret the original features as linear combination of features\n",
    " \n",
    "* PCA is often used as a tool for data visualization.\n",
    "    - View the data in lower dimensions\n",
    "\n",
    "* PCA can be applied to both linear and non-linear data (Kernel PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Geometry of PCA\n",
    "* Change of basis.\n",
    "![](pca.png)\n",
    "$$\\text{Figure 1. Principal Component Analysis}$$\n",
    " \n",
    "### Terminology\n",
    "\n",
    "#### Linear combination of the p features with constraints\n",
    "\n",
    "\n",
    "$$Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + ... + \\phi_{p1}X_p\\text{ subject to }\\sum^p_{j=1}\\phi^2_{j1} = 1$$\n",
    "$$Z_2 = \\phi_{12}X_1 + \\phi_{22}X_2 + ... + \\phi_{p2}X_p\\text{ subject to }\\sum^p_{j=1}\\phi^2_{j2} = 1$$\n",
    "$$\\vdots$$\n",
    "$$Z_d = \\phi_{1d}X_1 + \\phi_{2d}X_2 + ... + \\phi_{pd}X_p\\text{ subject to }\\sum^p_{j=1}\\phi^2_{jd} = 1$$\n",
    "\n",
    "* $\\phi_d = (\\phi_{1d},\\ldots,\\phi_{pd})$ is called a Principal Component loading vector\n",
    "* The loading vector $\\phi_1$ defines the direction in feature space along which the data varies the most. $\\phi_2$ is perpendicular to $\\phi_1$ and points to the direction with the second highest variance   \n",
    "* d <= p\n",
    "* The coefficients $\\phi_{1d},\\ldots,\\phi_{pd}$ are called the loadings\n",
    "\n",
    "* The values of the projection of the n data points onto the loading vectors are called the scores.\n",
    "* We constrain sum the loadings to 1, otherwise setting them to be arbitrarily large could result in an arbitrarily large variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intutition\n",
    " \n",
    "![](ToyPCAExample.png)\n",
    "\n",
    "$$\\text{Figure 2. Experimental Setup of System}$$\n",
    "\n",
    " \n",
    "### Physical Example \n",
    "\n",
    "* Imagine you are ignorant of the physical system\n",
    "    - you don't know that the underlying dynamics can be expressed as a function of a single variable x.\n",
    "* You decide to place cameras at arbitrary angles around system\n",
    "* Data will be noisy and redundant \n",
    "* Data\n",
    "    - Each camera reports x and y position\n",
    "    - 12000 samples (trials)\n",
    "    - p x n matrix with p = 6 measurement types an n = 12000 trials\n",
    "    - X is one column of measurement types (features, predictors)\n",
    "    \n",
    "$$ \n",
    "  X = \\begin{bmatrix}\n",
    "    x_A \\\\y_A   \\\\ x_B \\\\y_B   \\\\x_C \\\\y_C\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Main idea: Change of basis\n",
    "    - Goal: To compute the best basis to re-express the data in order to reduce redundancy and noise in the data\n",
    "    - How to find this new basis? \n",
    "        - Transform the data\n",
    "        - PX = Y\n",
    "            - P is a matrix that transforms X into Y.\n",
    "            - P is a rotation and a stretch of X into Y.\n",
    "            - The rows of P will be the set of new basis vectors for expressing the columns of X.\n",
    "    \n",
    "* Redundancy\n",
    "    - When multiple sensors (variables) contain the same information\n",
    "    - By changing basis we hope to reduce redundancy and noise\n",
    "    - The high redundancy in Figure 3c could be caused by having two of the cameras nearby (i.e. cameras B and C in Figure 2).\n",
    "    - We could combine r1 and r2 to a single variable that is a linear combination of the two. (i.e. we could just use one camera) \n",
    "        \n",
    "![](redundancy.png)\n",
    "$$\\text{Figure 3. Degrees of Redundancy}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covaiarnce Matrix (S)\n",
    "\n",
    "* Quantifies redundancy\n",
    "\n",
    "* Visualization for three variables\n",
    "\n",
    "|S |X|Y|Z|\n",
    "|:----:|:----:|:----:|:----:|\n",
    "|X|cov(x,x)  |cov(x,y)  | cov(x,z)   | \n",
    "|Y|cov(y,x)  |  cov(y,y)  | cov(y,z)   | \n",
    "|Z|cov(z,x) |cov(z,y)  |  cov(z,z)    | \n",
    "\n",
    "* cov(x,x) = var(x)\n",
    " \n",
    "#### Covariance matrix for our physical example \n",
    "* X is column-vector of measurements (6x1)\n",
    "* S is a square symmetric (6x6) matrix \n",
    "* Variance of each measurement is along the diagonal\n",
    "* Covariances between separate measurements are the off-diagonal terms\n",
    "    - A large term corresponds to Figure 3c\n",
    "    - A zero term corresponds to Figure 3a  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce Redundancy by Diagonalizing the Covariance Matrix\n",
    "    \n",
    "* To reduce redundancy, we would like each variable to co-vary as little as possible with other variables. \n",
    "    - i.e. covariances between separate measurements = zero\n",
    "    - a Diagonal matrix has zero on the off-diagonal terms\n",
    "* PCA is based on the easiest way to diagonalize the covariance matrix\n",
    "\n",
    "* Assume:\n",
    "    - 1). all basis vectors are perpendicular (orthonormal)\n",
    "    - 2). direction of largest variance is the \"most\" important\n",
    "* Select the direction in m-dimensional (measurement type) space along which variance in X is maximized (P1)\n",
    "* Find another direction which variance is maximized and is perpendicular to P1 (P2)\n",
    "* Continue until m-directions selected\n",
    " \n",
    "## PCA \n",
    "\n",
    "### Assumptions:\n",
    "\n",
    "- Linear - data is a linear combination of basis vectors\n",
    "- $X_i$s are Gaussian distributed\n",
    "- The directions of highest variability (i.e. the principal components) represent the interesting dynamics while those with lower variance represent noise\n",
    "- The principal components are orthogonal (i.e. perpendicular)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Algorithm\n",
    "\n",
    "1. Scale the data by centering (mean = 0) and scaling the stanard deviation to 1.  \n",
    "2. Create the covariance matrix of the scaled data.  \n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "4. Sort the eigenvalues and eigenvectors in decreasing order.\n",
    "5. Determine percent of variance explained to determine d (the number of principle components to keep)\n",
    "6. Construct the projection matrix P (of loading vectors) from the d eigenvectors\n",
    "7. Construct the d-dimensional feature subspace Y by projecting the scaled data onto P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "X = iris.iloc[:,0:4].values\n",
    "y = iris.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Scale the data by centering (mean = 0) and scaling the stanard deviation to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the covariance matrix of the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NumPy covariance matrix: \\n')\n",
    "cov_mat = np.cov(X_std.T)\n",
    "cov_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate the eigenvalues and eigenvectors of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)\n",
    "print('Eigenvectors \\n%s' %eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Check that the eigenvectors are orthonormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check orthonormal eigen vectors\n",
    "for i in range(4):\n",
    "    print(\"eigenvector: \",i)\n",
    "    for j in range(4):\n",
    "        print(j, round(eig_vecs[i].dot(eig_vecs[j]),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sort the eigenvalues and eigenvectors in decreasing order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(reverse=True)\n",
    "eig_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Determine percent of variance explained to determine d (the number of principle components to keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Variance explained: \",var_exp)\n",
    "print(\"Cummlative variance explained: \",cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((0,1,2,3,4),np.insert(cum_var_exp,0,0))\n",
    "plt.xlabel(\"Principal Component Number\")\n",
    "plt.ylabel(\"% Variance Explained\")\n",
    "\n",
    "\n",
    "d = 2 # Number of Principal Components\n",
    "print(\"Set number of Principal Componets = \",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Construct the projection matrix P from the d eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(eig_vals)\n",
    "P = np.zeros(d*n).reshape(n,d)\n",
    "for i in range(d):\n",
    "    for j in range(n):\n",
    "        P[j,i] = eig_pairs[i][1][j]\n",
    "\n",
    "print(\"Projection Matrix\")\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Construct the d-dimensional feature subspace Y by projecting the scaled data X onto P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X_std.dot(P) # XP\n",
    "d = {'x': Y[:,0],'y': Y[:,1],'Species': iris.loc[:,'species']}\n",
    "iris2 = pd.DataFrame(data = d)\n",
    "iris2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot('x','y', hue='Species' ,data = iris2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA function in sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess  Wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wine = pd.read_csv('Wine.csv')\n",
    "X = wine.iloc[:, 0:13].values\n",
    "y = wine.iloc[:, 13].values\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "# Scale\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_train_std.shape, X_test_std.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = None)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "X_train_pca.shape, X_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = pca.explained_variance_ratio_\n",
    "print(\"Ratio of Variance Explained:\")\n",
    "print(np.round(var_exp,3))\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cummulative Variance Explained\")\n",
    "print(np.round(cum_var_exp,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(cum_var_exp)+1)\n",
    "plt.plot(x,np.insert(cum_var_exp,0,0))\n",
    "plt.xlabel(\"Principal Component Number\")\n",
    "plt.ylabel(\"% Variance Explained\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use transformed feature array in a Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Apply PCA\n",
    "pca = PCA(n_components = 4)\n",
    "X_train_pca2 = pca.fit_transform(X_train)\n",
    "X_test_pca2 = pca.transform(X_test)\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "print(\"Variance Explained: \",var_exp)\n",
    "model = LogisticRegression(random_state = 0) #Regression with two features\n",
    "model.fit(X_train_pca2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test_pca2)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = np.trace(cm)/np.sum(cm)\n",
    "print(\"accuracy: \",round(accuracy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernal PCA\n",
    "\n",
    "* PCA for non-linear data\n",
    "* Use Kernel Trick like we did with Support Vector Machines\n",
    "    * Map to higher dimension to separate data with hyperplane\n",
    "    * Use Kernel functions which will compute the dot product in the mapped dimension \n",
    "        * Don't actually need to do the mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn non-kernel Principal Component Analysis \n",
    "\n",
    "#### Fit and transform the data into the top 2 components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize original and transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n",
    "\n",
    "ax1.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "ax1.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "ax2.scatter(X_pca[y == 0, 0], np.zeros((50, 1)) + 0.02, color='red', marker='^', alpha=0.5) # LD1 only\n",
    "ax2.scatter(X_pca[y == 1, 0], np.zeros((50, 1)) - 0.02, color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kermel PCS Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import exp\n",
    "from scipy.linalg import eigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute pairwise squared distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dists = pdist(X, 'sqeuclidean')\n",
    "print(sq_dists.shape) \n",
    "mat_sq_dists = squareform(sq_dists) # Convert pairwise distances into a square matrix.\n",
    "mat_sq_dists.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Function (RBF) Kernel\n",
    "\n",
    "<div style=\"font-size: 115%;\"> \n",
    "$$K(\\vec{x_i},\\vec{x_j}) = exp(-\\gamma||\\vec{x_i} - \\vec{_j}||^2_2) \\text{ where }\\gamma > 0$$\n",
    "</div>\n",
    "\n",
    "* gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the symmetric kernel matrix.\n",
    "gamma = 15\n",
    "K = exp(-gamma * mat_sq_dists)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center the kernel matrix.\n",
    "\n",
    "*  Centering Trick\n",
    " \n",
    "#### $K_{centered} = K − 1_n K − K1_n + 1_nK1_n = (I − 1_n)K(I − 1_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = K.shape[0]\n",
    "one_n = np.ones((N, N)) / N\n",
    "print(one_n.shape, one_n[0,0])\n",
    "\n",
    "K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n",
    "print(K.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain eigenpairs from the centered kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.linalg.eigh returns them in ascending order\n",
    "eigvals, eigvecs = eigh(K)\n",
    "eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n",
    "\n",
    "eigvals.shape,eigvecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect the top k eigenvectors into projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "X_kpca = np.column_stack((eigvecs[:, i] for i in range(n_components)))\n",
    "X_kpca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_kpca(X_kpca):\n",
    "    fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2, figsize=(7,3))\n",
    "    ax1.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "    ax1.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "    ax2.scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02, color='red', marker='^', alpha=0.5)\n",
    "    ax2.scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02, color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax2.set_ylim([-1, 1])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_xlabel('PC1')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kpca(X_kpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernal PCA in sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components = 2, kernel = 'rbf', gamma = 15)\n",
    "X_kpca2 = kpca.fit_transform(X)\n",
    "X_kpca2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kpca(X_kpca2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA on Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "X = iris.iloc[:, 0:4].values\n",
    "y = iris.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0,stratify=y)\n",
    "\n",
    "# Scale\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_train_std.shape,X_test_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Kernel PCA\n",
    "kpca = KernelPCA(n_components = None, kernel = 'rbf')\n",
    "X_train_kpca = kpca.fit_transform(X_train_std)\n",
    "X_test_kpca = kpca.transform(X_test_std)\n",
    "\n",
    "var_exp = kpca.lambdas_/np.sum(kpca.lambdas_) # eigenvalues in decreasing order, normalize\n",
    "\n",
    "X_train_kpca.shape,X_test_kpca.shape,var_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "cum_var_exp[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for d in range(1,6): # number of components\n",
    "    # Apply kpca\n",
    "    kpca = KernelPCA(n_components = d, kernel = 'rbf')\n",
    "    X_train_kpca = kpca.fit_transform(X_train)\n",
    "    X_test_kpca = kpca.transform(X_test)\n",
    "\n",
    "\n",
    "    # Fit\n",
    "    m = LogisticRegression(random_state = 0)\n",
    "    m.fit(X_train_kpca, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = m.predict(X_test_kpca)\n",
    "\n",
    "    # Make the Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = np.trace(cm)/np.sum(cm)\n",
    "    print('d = ',d,' Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### References\n",
    "\n",
    "Figure 1 source: \"An Introduction to Statistical Learning, with applications in R\"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani \n",
    "\n",
    "Jon Shlens (2003) A TUTORIAL ON PRINCIPAL COMPONENT ANALYSIS - Derivation, Discussion and\n",
    "Singular Value Decomposition\n",
    "\n",
    "Principal Component Analysis in Python: https://plot.ly/ipython-notebooks/principal-component-analysis/\n",
    "\n",
    "Raschka,Sebastin & Mirjalili, Vahid (2017). Python Machine Learning, 2nd Edition, Packt Publishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "## Solving PCA: Eigenvectors of Covariance Matrix\n",
    " \n",
    "* X is the original matrix adjusted so column means = 0\n",
    " \n",
    "* Find some orthonormal matrix P where PX = Y\n",
    "    - such that the covariance matrix $S_Y = \\frac{1}{n-1}YY^T = D$ (i.e. is diagonalized)\n",
    "    - P will contain the principal components.\n",
    "    \n",
    "$$ S_Y = \\frac{1}{n-1}YY^T \\\\\n",
    "      = \\frac{1}{n-1}(PX)(PX)^T \\\\\n",
    "      = \\frac{1}{n-1}PXX^TP^T \\\\\n",
    "      = \\frac{1}{n-1}PAP^T\n",
    "$$\n",
    "      \n",
    "* Linear Algebra Theorems\n",
    "    - For an orthogonal (i.e. pairwise dot product of columns = 0) matrix A, $A^{-1} = A^T$ \n",
    "    - Note: A = $XX^T$ is symmetric\n",
    "    - A symmetric matrix can be diagonalized A by an orthogonal matrix of its eigenvectors $A = EDE^T$\n",
    "      \n",
    "* Select P to be the matrix of eigenvectors of $A = XX^T$\n",
    "    - P = $E^T$\n",
    "\n",
    "$$ S_Y = \\frac{1}{n-1}P(EDE^T)P^T \\\\\n",
    "      = \\frac{1}{n-1}P(P^TDP)P^T \\\\\n",
    "      = \\frac{1}{n-1}(PP^T)D(PP^T) \\\\\n",
    "      = \\frac{1}{n-1}(PP^{-1})D(PP^{-1}) \\\\\n",
    "      = \\frac{1}{n-1}D\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "372px",
    "left": "809px",
    "right": "20px",
    "top": "13px",
    "width": "492px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
