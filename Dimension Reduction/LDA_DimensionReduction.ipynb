{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction by Feature Extraction \n",
    " \n",
    "* Transforms the features into a lower dimensional feature space.\n",
    "    - Information about the dataset is compressed.\n",
    "    - Feature selection keeps the same features, feature extraction creates new features\n",
    "    - The transformed features then can be used in regression and classification tasks\n",
    "\n",
    "* Methods\n",
    "    - Linear Discriminant Analysis (Supervised)\n",
    "    - Principal Component Analysis (Unsupervised)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wine data\n",
    "\n",
    "* Class wine into one of three classes \n",
    "* 13 numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('Wine.csv')\n",
    "wine.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.iloc[:, 0:13].values\n",
    "y = wine.iloc[:, 13].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234, stratify = y)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "* LDA is used for classification and feature extraction\n",
    "    - A **supervised** dimensionality reduction technique for maximizing class separability\n",
    "        - Uses information provided by the class labels to aid discriminating\n",
    "    - Extracted feature vectors indicate the direction of maximimum class variability and separation\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* The classes have Normal distributions  \n",
    "* The classes have identical covariance matrices\n",
    "* The features are statistically independent of each other.\n",
    "* LDA for dimension reduction can work fairly well even if these assumptions are violated.\n",
    "    - LDA classification is also fairly robust to the distribution of the classes\n",
    "\n",
    "![](LDA.png)\n",
    "$$\\text{Figure 1. Two Classes}$$\n",
    "\n",
    "* LDA computes the directions (“linear discriminants”) that will represent the axes that that maximize the separation between multiple classes.\n",
    "\n",
    "* Projecting the data onto the X-axis maximizes the class separation.\n",
    "    - LD1 linear discriminant on the x-axis (LD 1) would do a good job of separating the two normal distributed classes. \n",
    "* The linear discriminant LD 2 captures a lot of the variance in the dataset but would fail as a good linear discriminant (i.e. projection) since it does not capture any of the information that discriminates the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Algorithm\n",
    "\n",
    "* LDA computes means and scatter matricies on standardize data. The eigenvalues and eigenvectors of the combined matrices are used to project the data onto the lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Create mean of features grouped by class\n",
    "\n",
    "*  LDA takes class label information into account, which is represented in the form of the mean vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVs = [np.mean(X_train_std[y_train == label], axis=0) for label in [1,2,3]]\n",
    "MVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Create the Within Class Scatter Matrix\n",
    "\n",
    "* Individual Class scaled scatter matrix is just the covariance matrix $S_i$\n",
    "$$S_w = \\sum_CS_c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_train.shape[1] # number of features\n",
    "S_W = np.zeros((d, d))\n",
    "for label in range(1, 4):\n",
    "    class_scatter = np.cov(X_train_std[y_train == label].T)\n",
    "    S_W += class_scatter\n",
    "\n",
    "S_W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Create the Between Class Scatter Matrix\n",
    "\n",
    "$$S_B = \\sum_{i = 1}^Cn_i(m_i - m)(m_i - m)^T$$\n",
    "\n",
    "$n_i$ is the number of observations in class i  \n",
    "$m_i$ is the Class mean  \n",
    "m is the overall mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean = np.mean(X_train_std, axis=0)\n",
    "print(overall_mean.shape)\n",
    "S_B = np.zeros((d, d))\n",
    "for i, mv in enumerate(MVs):\n",
    "    n = X_train[y_train == i + 1, :].shape[0]\n",
    "    mv = mv.reshape(d, 1)  # make column vector\n",
    "    overall_mean = overall_mean.reshape(d, 1)  # make column vector\n",
    "    S_B += n * (mv - overall_mean).dot((mv - overall_mean).T) # (class mean - overall Mean) squared\n",
    "S_B.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Compute the eigenvectors and corresponding eigenvalues of the matrix  $S_W^{-1}S_B$ (i.e. $S_B/S_W$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "               for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)\n",
    "eigen_vals = list(map(lambda x: x[0],eigen_pairs))\n",
    "eigen_vecs = list(map(lambda x: x[1],eigen_pairs))\n",
    "list(eigen_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Choose the k eigenvectors that correspond to the k largest eigenvalues.\n",
    "\n",
    "* Construct W:  a d × k transformation matrix  \n",
    "* The eigenvectors are the columns W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros((13,2))\n",
    "W = np.hstack((np.array(eigen_vecs[0]).reshape(-1,1).real,\n",
    "               np.array(eigen_vecs[1]).reshape(-1,1).real))\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Project the data onto the new feature subspace using the transformation matrix W, 142x13 X 13x2 = 142x2\n",
    "$$ X_{new} = X_{train} \\cdot{W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda = X_train_std.dot(W)\n",
    "X_train_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data in new feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_lda[y_train == l, 0],\n",
    "                X_train_lda[y_train == l, 1] * (-1),\n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA for Dimension Reduction in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and transform the Training data to 2-dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_lda = LDA(n_components = 2) # Defaults to full number of dimensions so need to set to the reduced space\n",
    "X_train_lda2 = model_lda.fit_transform(X_train_std, y_train) # Fit and transform to use in Logistic Regression\n",
    "X_test_lda2 = model_lda.transform(X_test_std)\n",
    "X_train_lda2.shape,X_test_lda2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_lda2[y_train == l, 0],\n",
    "                X_train_lda2[y_train == l, 1] ,\n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Logistic Regression model using all thirteen prdictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "\n",
    "model_lr = LogisticRegression(random_state = 0)\n",
    "model_lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = model_lr.predict(X_test_std)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f'\\nAccuracy: {np.trace(cm)/np.sum(cm)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a Logistic Regression model with the reduced Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to reduced feature space\n",
    "\n",
    "model_lr2 = LogisticRegression(random_state = 0)\n",
    "model_lr2.fit(X_train_lda2, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = model_lr2.predict(X_test_lda2)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f'\\nAccuracy: {np.trace(cm)/np.sum(cm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "372px",
    "left": "809px",
    "right": "20px",
    "top": "13px",
    "width": "492px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
