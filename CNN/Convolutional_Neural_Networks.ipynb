{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "* Yann LeCun, 1998\n",
    "    - LeNet 5: recognizing hand-written digits and words\n",
    "    - 99.2% accuracy on MINST dataset\n",
    "\n",
    "* AlexNet,Alex Krizhevsky,Ilya Sutskever, Geoffrey Hinton, 2012\n",
    "\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "\n",
    "![](Hinton1.png)\n",
    "\n",
    "* ImageNet Challenge\n",
    "    - 1000 Categories\n",
    "    - 1.2M Training\n",
    "    100,000 Test\n",
    "* 1st Place with Top five 15.3% error rate\n",
    "    - 26.2% error rate for second best entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architecture\n",
    "\n",
    "![](CNN_Architecture.jpg)\n",
    "![](FullCNN1.png)\n",
    "\n",
    "Source: towardsdatascience.com\n",
    "\n",
    "### Feature Learning\n",
    "\n",
    "* Learns which features are important\n",
    "\n",
    "* Replaced manual feature engineering\n",
    "    - Manual feature engineering is time consuming and prone to human biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "#### Why Convolution?\n",
    "\n",
    "* Location invariance\n",
    "* Locality: An object is identified by its local context\n",
    "\n",
    "\n",
    "#### Function\n",
    "* Combines two functions in time\n",
    "* Shape of one function modified by the other\n",
    "\n",
    "$$(f*g)(t) = \\int_{-\\infty}^{\\infty}f(\\tau)(t - \\tau)d\\tau$$\n",
    "\n",
    "http://mathworld.wolfram.com/Convolution.html\n",
    "\n",
    "\n",
    "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolution.html\n",
    "\n",
    "* Technically the convolution operation in a CNN is Cross-Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Image\n",
    "\n",
    "* Typically:\n",
    "    - 256x256 for grey scale images\n",
    "    - 256x256x3 for color (RGB) images\n",
    "* \n",
    "\n",
    "![](Convolution1.png)\n",
    "\n",
    "* Stride - how much to move the feature detector sideways and down\n",
    "    - 2 is a popular choice\n",
    "    - Reduce dimensionality\n",
    "    \n",
    "* Padding - Put a border of zeros around input image\n",
    "\n",
    "#### Feature Detector (Kernel, Filter)\n",
    "\n",
    "* Convolves the kernel with the image\n",
    "\n",
    "* Detects different features in the image\n",
    "    - Edge\n",
    "    - Line\n",
    "* Alters the image\n",
    "    - Blur (averages)\n",
    "    \n",
    "* Feature detectors are organized in layers (e.g. 32 layers, 1 feature detector per layer)\n",
    "* **CNNs learn the best feature detectors in the same way the weights are learned**\n",
    "\n",
    "http://setosa.io/ev/image-kernels/\n",
    "    \n",
    "#### Feature Map\n",
    "\n",
    "* Reduced form of image\n",
    "* Lost some information but extracted important information (e.g. the features)\n",
    "\n",
    "![](Convolution2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Correlation Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    print(X.shape,K.shape,Y.shape)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.Tensor([[0, 1], [2, 3]])\n",
    "print(X)\n",
    "print(K)\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Detection\n",
    "\n",
    "* 1.,0. is a white to black edge, 0., 1. is a black to white edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.Tensor([[1, -1]])  ## Detects vertical edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn edge detection kernel K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "Y = corr2d(X, K)\n",
    "\n",
    "# 1 input channel, 1 output channel, kernel shape of (1, 2)\n",
    "\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2),bias=False) #Ignoring bias\n",
    "\n",
    "# The 2-d convolutional layer uses four-dimensional input and output\n",
    "# number of examples , number of channels, height, width)\n",
    "\n",
    "X_ = X.reshape((1, 1, 6, 8))\n",
    "Y_ = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "alpha = 0.03  # Learning Rate\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    conv2d.zero_grad() # Zero gradients\n",
    "    Y_hat = conv2d(X_) # Forward pass\n",
    "    loss = ((Y_hat - Y_) ** 2).sum()  # Calc Loss\n",
    "    loss.backward() # Calc Gradients\n",
    "    conv2d.weight.data[:] -= alpha * conv2d.weight.grad # Gradient Descient\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print('batch %d, loss %.3f' % (epoch + 1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv2d.weight.data.shape)\n",
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "\n",
    "#### Relu\n",
    "\n",
    "* Applied to increase non-linearity in network\n",
    "    - Convolution step is linear\n",
    "* Applied element-wise to feature map to remove negative values\n",
    "    - e.g. remove all the black pixels (sharpens the border between objects) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling (Downsampling)\n",
    "\n",
    "* Spatial Invariance\n",
    "\n",
    "* Types\n",
    "    - Max\n",
    "    - Min\n",
    "    - Sum\n",
    "    - Mean\n",
    "    - Subsampling\n",
    "\n",
    "#### Max Pooling \n",
    "\n",
    "* It is the most popular\n",
    "    - 2x2\n",
    "    - Stride = 2\n",
    "\n",
    "![](MaxPooling1.png)\n",
    "\n",
    "\n",
    "![](MaxPooling2.png)\n",
    "\n",
    "* Removes information but preserves features\n",
    "* Accounts for distortions\n",
    "* Reduced size therefore fewer parameters\n",
    "    - helps reduce overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float32)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling and Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch MaxPool2d \n",
    "\n",
    "* By default the stride has the same shape as the pooling window shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d(3) # Pooling and stride shape is 3x3, no padding\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3)) # pad should be smaller than half of kernel size\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat((X, X + 1), dim=1)\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Convolution and Pooling\n",
    "\n",
    "http://scs.ryerson.ca/~aharley/vis/conv/flat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening\n",
    "\n",
    "* Convert final pooled feature maps to a 1-d array\n",
    "    - by row\n",
    "![](flattening.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected ANN\n",
    "\n",
    "![](FullAnn2.png)\n",
    "\n",
    "* Input vector of  features\n",
    "* Combines features\n",
    "* Activation in output layer: \n",
    "    - Softmax when doing classification\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "* Cross-entropy\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "* Weights adjusted\n",
    "\n",
    "* **Feature detectors are adjusted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Cross-Entropy\n",
    "\n",
    "* Only for Classification\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "#### for j = 1.2,...,k\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ f(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n",
    "</div>\n",
    "\n",
    "* Generalization of logistic (i.e. sigmoid) to K classes. \n",
    "    - Outputs probabilities of the classes\n",
    "    \n",
    "#### Cross-Entropy\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$H(p,q) = -\\sum_x p(x)log(q(x))$$\n",
    "</div>\n",
    "\n",
    "* The output layer represents a distribution (q above) from the softmax activation function\n",
    "\n",
    "* Cross-entropy indicates the distance between the q distribution in the output layer and the labeled distribution p.\n",
    "\n",
    "* When the targets are 0 and 1, cross-entropy tends to allow errors to propagate backwards in order to change weights even when the error is small (because of the log term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "* Regularization technique\n",
    "* At each training epoch, individual units with their incoming and outgoing edges are randomly dropped from the network structure.\n",
    "\n",
    "* This ovoids overfitting by reducing the interdependency of the units.\n",
    "\n",
    "![](Dropout.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
