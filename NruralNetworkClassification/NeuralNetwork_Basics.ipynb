{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network Logistic Regression\n",
    " \n",
    "\n",
    "* Artificial Neutral Networks are biologically  inspired mathematical objects that use function composition to approximate any computable function.\n",
    "\n",
    "#### Components\n",
    "\n",
    "* Architecture\n",
    "    - Layers: Contain nodes\n",
    "    - Weights: link nodes in the layers\n",
    "* Forward Pass\n",
    "    - Function Composition: Moving forward from input layer to output layer producing target value(s)\n",
    "    - Activation functions (non-linearity)\n",
    "    \n",
    "* Backward Pass\n",
    "    - Loss Function\n",
    "        - Compare output with label, difference is the amount of error\n",
    "        - Squared Loss\n",
    "        - Cross-entropy loss\n",
    "    - Back propagation: Propagate the error backwards through the network\n",
    "    - Optimizers adjusting the weights \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture (Multilayer Perceptron)\n",
    "\n",
    "#### Fully-connected Feedforward Nework\n",
    "\n",
    "* A node at each layer to connected to all the nodes in the next layer.\n",
    "\n",
    "* No connections within a layer\n",
    "\n",
    "\n",
    "![](Perceptron1.png)\n",
    "$$\\text{Figure 1. Rosenblatt's Perceptron}$$\n",
    "\n",
    "\n",
    "![](MLP1.png)\n",
    "$$\\text{Figure 2. Multilayer Perceptron}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Input layer\n",
    "    - m features, ${x_i, i = 1,...,m}$\n",
    "    - One input node per feature\n",
    "* Hidden Layer\n",
    "    - Each layer transforms via an **activation** function f the weighted sum of its input $a_i = f(\\sum x_iw_i)$\n",
    "    - Width of layer is number of nodes in layer\n",
    "    - Depth of network is number of layers\n",
    "    - Layers can have different activation functions\n",
    "* Output Layer\n",
    "    - Final transformation to target outcome\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "#### Sigmoid (Logistic)\n",
    "\n",
    "* Range (0,1)\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "</div>\n",
    "\n",
    "* Derivative  \n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ \\frac{d}{dz}\\sigma(z) = -(1 + e^{-z})^{-2}(-e^{-z}) =  \\frac{1}{1 + e^{-z}}\\left(1 - \\frac{1}{1 + e^{-z}}\\right) =  \\sigma(z)(1 - \\sigma(z))$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "# Derivative of Sigmoid or Logistic function\n",
    "def D_Sigmoid(z): \n",
    "    s = Sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "z = np.linspace(-10,10,100)\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.plot(z,Sigmoid(z))\n",
    "ax1.set_title('Sigmoid(z)')\n",
    "ax2.plot(z,D_Sigmoid(z))\n",
    "ax2.set_title('Derivative of Sigmoid');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Non-linear\n",
    "* In domain (-3, +3) it is very steep, pushing y values to the extremes\n",
    "    - Good for classification\n",
    "* Outside of (-3,+3) gradient is very small (vanishes)\n",
    "* Range of sigmoid is (0,1) therefore not symmetric around 0\n",
    "    - Only positive outputs which is not always desirable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperbolic Tangent\n",
    "\n",
    "* Range is (-1,1)\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ tanh(z) = \\frac{sinh(z)}{cosh(z)} = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = \\frac{2}{1 + e^{-2z}} - 1$$\n",
    "</div>\n",
    "\n",
    "* Derivative:\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ \\frac{d}{dz}tanh(z) = 1 - tanh^2(z)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.e**z - np.e**(-z))/(np.e**z + np.e**(-z))\n",
    "\n",
    "def D_tanh(z):\n",
    "    return (1 - tanh(z)**2)\n",
    "\n",
    "z = np.linspace(-5,5,100)\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.plot(z,tanh(z))\n",
    "ax1.set_title('tanh(z)')\n",
    "ax2.plot(z,D_tanh(z))\n",
    "ax2.set_title('Derivative of tanh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scaled version of sigmoid: tanh(z)=2sigmoid(2z)-1\n",
    "\n",
    "* Range is (-1, 1), symmetric about the origin\n",
    "\n",
    "* Still have vanishing gradient problem\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit (ReLU)\n",
    "\n",
    "* Most popular activation function for hidden layers\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ ReLU = max(0,z)$$\n",
    "\n",
    "$$ \\frac{d}{dz}ReLU(z) = 1 \\text{ if } z > 0 \\text{ else } 0 $$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return z * (z > 0)\n",
    "\n",
    "def D_ReLU(z):\n",
    "    return 1 * (z > 0)\n",
    "\n",
    "z = np.linspace(-5,5,100)\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.plot(z,ReLU(z))\n",
    "ax1.set_title('ReLU(z)')\n",
    "ax2.plot(z,D_ReLU(z))\n",
    "ax2.set_title('Derivative of ReLU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Non-linear\n",
    "\n",
    "* It does not saturate for large positive values\n",
    "\n",
    "* Easy to compute\n",
    "\n",
    "* Derivative is 0 or 1\n",
    "    - If gradient = 1 then doesn't vanish when multiplied together\n",
    "    - If gradient = 0 then the weights will not be updated during backprop\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU\n",
    "\n",
    "* Improved version of ReLU, solves the problem of updating the weights for inputs less than zero\n",
    "     - Replaces the horizontal part of the function with a non-zero non-horizontal line\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ LeakyReLU =  a*z \\text{ if } z < 0 \\text{ else } z$$\n",
    "$$ \\frac{d}{dz}LeakyReLU(z) = 1 \\text{ if } z > 0 \\text{ else } a $$\n",
    "</div>   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_ReLU(z, a = 0.01):\n",
    "    return np.concatenate((a*z[z<0],z[z>= 0]))\n",
    "\n",
    "def D_L_ReLU(z, a = 0.01):\n",
    "    return np.concatenate((a * np.ones(len(z[z<0])), np.ones(len(z[z>= 0]))))\n",
    "\n",
    "z = np.linspace(-5,5,20)\n",
    "print(L_ReLU(z))\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "ax1.plot(z,L_ReLU(z))\n",
    "ax1.set_title('Leaky ReLU(z)')\n",
    "ax2.plot(z,D_L_ReLU(z))\n",
    "ax2.set_title('Derivative of Leaky ReLU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "* Multiclass\n",
    "\n",
    "* Outputs the probability that the input belongs to a particular class:\n",
    "\n",
    "#### for j = 1.2,...,k\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ Softmax(z)_j =S(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}} $$\n",
    "</div>\n",
    "\n",
    "* Derivative\n",
    "\n",
    "$$\\frac{d}{dz}S(z)_j = S(z)_j(\\delta_{jk} - S(z)_k)$$\n",
    "\n",
    "$$ \\delta \\text{ is the Kronecker delta function}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z  = np.array([1.5,3.2,.75] )\n",
    "\n",
    "def Softmax(z):\n",
    "    ez = np.e**z\n",
    "    return ez/np.sum(ez)\n",
    "s = Softmax(z)\n",
    "print(f'Softmax(z) = {s}')   \n",
    "np.sum(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass: Learning the weights\n",
    "\n",
    "* Calculate the Loss (i.e. Error, Cost) at the end of the forward pass and then propagate the error backwards though the layers to the input.\n",
    "\n",
    "* Changing (i.e. updating) the weights is the only way to affect the loss. We use gradient descent to update the weights in neural networks.\n",
    "\n",
    "* Gradient descent: Minimize the loss function by iteratively moving in the direction of the negative of the gradient. \n",
    "\n",
    "* Training the network: Repeat propagating the error backwards and adjusting the weights until the loss is within some tolerance.\n",
    "\n",
    "#### Loss Functions\n",
    "\n",
    "* Loss functions\n",
    "    - The function to be minimized during training\n",
    "    - Function of the output value and target value\n",
    "    - Squared error\n",
    "        - Used in Linear Regression\n",
    "    - Cross-entropy loss (log loss): -(ylog(p) + (1-p)(log(1-p))\n",
    "        - measures the performance of a classification model that outputs a probability.\n",
    "        - increases as the predicted probability diverges from the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryCrossEntropy(yHat, y):\n",
    "    if y == 1:\n",
    "      return -log(yHat)\n",
    "    else:\n",
    "      return -log(1 - yHat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Backpropagation and Gradient Descent \n",
    "\n",
    "* Three type of Gradient Descent\n",
    "    - **Batch** - Update all trials (observations) each epoch\n",
    "    - **Stochastic** - randomly select one observation to update each epoch\n",
    "    - **Mini Batch** - Choose a batch (e.g.32) observations to update each epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers\n",
    "\n",
    "* Stochastic Gradient Descent (SGD)\n",
    "* Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of Backprop and gradient descent for a  simple Neural Network\n",
    "\n",
    "* For illustrative purposes using 1 Hidden layer of width 1 with sigmoid activation function for hidden and output layers\n",
    "\n",
    "![](Simple_NN1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot Product: Multiply vectors to get a scalar\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$  x = (x_1,x_2,...,x_n), w = (w_1,w_2,...,w_n)$$\n",
    "</div>    \n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$  x\\centerdot{ w} = \\sum_i^n x_i*w_i $$\n",
    "</div>\n",
    "\n",
    "Loss Function: L = $\\frac{1}{2}(Y - y_{out})^2$  \n",
    "\n",
    "Derivative of Loss Function: $D(L) = -(Y - y_{out})$\n",
    "\n",
    "Sigmoid Function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Derivative of Sigmoid: $D(\\sigma(z)) = \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "\n",
    "$$h_{in} = X \\cdot w_1,\\text{ } h_{out} = \\sigma(h_{in})$$\n",
    "\n",
    "$$y_{in} = h_{out} \\cdot w_2,\\text{ } y_{out} = \\sigma(y_{in})$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the weights (Gradient Descent)\n",
    "\n",
    "* $\\gamma$ is the learning rate\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ w_1^{(t)} = w_1^{(t-1)} - \\gamma\\frac{\\partial{L}}{\\partial{w_1^{(t-1)}}}$$\n",
    "$$ w_2^{(t)} = w_2^{(t-1)} - \\gamma\\frac{\\partial{L}}{\\partial{w_2^{(t-1)}}}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial derivative of the Loss with respect to (wrt) the weights\n",
    "\n",
    "* Heavy use of chain rule\n",
    "\n",
    "* Weight for Output Layer\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$\\frac{\\partial{L}}{\\partial{w_2}} = \\frac{\\partial{L}}{\\partial{y_{out}}}\\frac{\\partial{y_{out}}}{\\partial{y_{in}}} \\frac{\\partial{y_{in}}}{\\partial{w_2}} = -(Y - y_{out})(y_{out}(1 - y_{out}))h_{out}$$\n",
    "</div>\n",
    "\n",
    "* Weight for Hidden Layer\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$\\frac{\\partial{L}}{\\partial{w_1}} = \\frac{\\partial{L}}{\\partial{y_{out}}}\\frac{\\partial{y_{out}}}{\\partial{y_{in}}} \\frac{\\partial{y_{in}}}{\\partial{h_{out}}} \\frac{\\partial{h_{out}}}{\\partial{h_{in}}} \\frac{\\partial{h_{in}}}{\\partial{w_1}} =  -(Y - y_{out})(y_{out}(1 - y_{out}))w_2(h_{out}(1 - h_{out}))X$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions and derivatives\n",
    "#   Sigmoid or Logistic function\n",
    "def Sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "# Derivative of Sigmoid or Logistic function\n",
    "def D_Sigmoid(z): \n",
    "    s = Sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "#\n",
    "# Loss Function\n",
    "def Loss(y,yhat):\n",
    "    return (1/2)*(y - yhat)**2\n",
    "\n",
    "def D_Loss(y, yhat):\n",
    "    return -(y - yhat)\n",
    "\n",
    "# Dot Product\n",
    "def Dot(x,w):\n",
    "    return np.dot(x,w)\n",
    "\n",
    "def D_Dot(x,w):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Simple_NN1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 115%;\">\n",
    "$$\\sigma(W_2\\cdot{\\sigma(W_1\\cdot{X})})$$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Epoch(w_1,w_2,X,Y):\n",
    "    \n",
    "    # Proprogate Forward\n",
    "    h_in = Dot(X,w_1)\n",
    "    h_out = Sigmoid(h_in)\n",
    "    y_in = Dot(h_out,w_2)\n",
    "    y_out = Sigmoid(y_in)\n",
    "    \n",
    "    # Backpropagation\n",
    "        \n",
    "    # Calculate the change in the error due to the weight in the second layer\n",
    "        # D_Loss(Y, y_out) =  -(Y - y_out)* \n",
    "        # D_Sigmoid(y_in) =  (y_out(1 - y_out))\n",
    "    delta_y = -(Y - y_out) * (y_out * (1 - y_out))\n",
    "        # D_Dot(h_out,w_2) = h_out\n",
    "    error_w2 = h_out.T.dot(delta_y) \n",
    "    #print(delta_y.shape,h_out.shape,error_w2.shape)\n",
    "        # Calculate the change in the error due to the weight in weights in the first layer\n",
    "        # D_Dot(w_2, y_in) = w_2   \n",
    "        # D_Sigmoid(h_in) = h_out(1 - h_out) \n",
    "    delta_h = delta_y.dot(w_2.T) * (h_out * (1 - h_out))\n",
    "        # D_Dot(X,w_1) = X       \n",
    "    \n",
    "    error_w1 = X.T.dot(delta_h)\n",
    "    #print(delta_h.shape,X.shape,error_w1.shape)\n",
    "    \n",
    "    # Update weights\n",
    "    w_2 = w_2 - gamma * error_w2\n",
    "    w_1 = w_1 - gamma * error_w1\n",
    "    return w_1,w_2, y_out        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One sample,  three features, single output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](NN1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Data: 1 trial, 3 features\n",
    "X = np.array([[.2, .4, .6]])\n",
    "Y = np.array([[.8]])\n",
    "\n",
    "# Weight Matricies\n",
    "w_1 = 2*np.random.random((3,1)) - 1\n",
    "w_2 = 2*np.random.random((1,1)) - 1\n",
    "\n",
    "Max_Epoch = 2000\n",
    "tolerance = 0.001 \n",
    "gamma = 80\n",
    "Converged = np.isclose(Y,0,tolerance)\n",
    "cnt = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (not Converged.all()) and (cnt < Max_Epoch):\n",
    "    cnt += 1\n",
    "    w_1,w_2,y = One_Epoch(w_1,w_2,X,Y)\n",
    "    # Check if Converged\n",
    "    Converged = np.isclose(Y,y,tolerance)\n",
    "    \n",
    "mse = np.mean((Y - y)**2) \n",
    "print(f\"Output: {y} MSE: {mse}  Epoch {cnt}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# 4 trials, 3 features\n",
    "X = np.array([[.2, .4, .6],   \n",
    "              [.3, .3, .1],\n",
    "              [.5, .4, .7],\n",
    "              [.8, .4, .1]])\n",
    "N = X.shape[0]\n",
    "Y = np.array([[.8],[.2], [.4], [.6]])\n",
    "w_1 = 2*np.random.random((3,N)) - 1\n",
    "w_2 = 2*np.random.random((N,1)) - 1\n",
    "\n",
    "gamma = 50\n",
    "tolerance = 0.001\n",
    "Max_Epoch = 40000\n",
    "Converged = np.isclose(Y,0)\n",
    "cnt = 0 \n",
    "\n",
    "while (not Converged.all()) and (cnt < Max_Epoch):\n",
    "    cnt += 1\n",
    "    w_1,w_2,y = One_Epoch(w_1,w_2,X,Y)\n",
    "    # Check if Converged\n",
    "    Converged = np.isclose(Y,y,tolerance)\n",
    "    \n",
    "mse = np.mean((Y - y)**2) \n",
    "print(f\"Output: \\n {y} \\n \\n MSE: {mse}  Epoch {cnt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trials = np.random.permutation(range(4))\n",
    "print(\"Trials: \", trials)\n",
    "\n",
    "Data = np.array([[.2, .4, .6],\n",
    "              [.3, .3, .1],\n",
    "              [.5, .4, .7],\n",
    "              [.8, .4, .1]])\n",
    "\n",
    "Labels = np.array([[.8],[.2], [.4], [.6]])\n",
    "\n",
    "w_1 = 2*np.random.random((3,1)) - 1\n",
    "w_2 = 2*np.random.random((1,1)) - 1\n",
    "\n",
    "gamma = 80\n",
    "\n",
    "for tr in trials:\n",
    "    X = Data[tr,:].reshape(1,-1)\n",
    "    Y = Labels[tr,:].reshape(1,1)\n",
    "    \n",
    "    tolerance = 0.001\n",
    "    Max_Epoch = 40000\n",
    "    Converged = np.isclose(Y,0)\n",
    "    cnt = 0 \n",
    "\n",
    "    while (not Converged.all()) and (cnt < Max_Epoch):\n",
    "        cnt += 1\n",
    "        w_1,w_2,y = One_Epoch(w_1,w_2,X,Y)\n",
    "        # Check if Converged\n",
    "        Converged = np.isclose(Y,y,tolerance)\n",
    "    \n",
    "    mse = np.mean((Y - y)**2) \n",
    "    print(f\"Trial {tr} \\n Output: \\n {y} \\n \\n MSE: {mse}  Epoch {cnt}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
