{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "#### Sequence Learning\n",
    "\n",
    "* Input not independent\n",
    "    - e.g. auto completion\n",
    "* Input may not be of fixed size\n",
    "\n",
    "* To process a sequence the ability to \"remember\" previous events in the sequence\n",
    "\n",
    "* RNNs unfold in time to process events that occur in a sequence\n",
    "\n",
    "![](RNN.png)\n",
    "$$\\text{Figure 1. RNN unfolding in time}$$\n",
    "\n",
    "#### Output\n",
    "\n",
    "$$y_i = f(x_i,s_{i-1},W,U,V,b,c)$$\n",
    "\n",
    "i - time step  \n",
    "$x_i$ - input  \n",
    "$s_i$ - hidden states  \n",
    "U,W,V - Weight Matrices  \n",
    "b,c - bias  \n",
    "\n",
    "#### Activation \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$s_i = \\sigma(U_{x_{i}} + W_{s_{i-1}} + b)\\text{ or } tanh(U_{x_{i}} + W_{s_{i-1}} + b)$$\n",
    "$$y_i = Softmax(V_{s_i} + c)$$\n",
    "</div>\n",
    "\n",
    "\n",
    "    \n",
    "#### Why Recurrent?  \n",
    "\n",
    "* RNNs perform the same function for every element of a sequence\n",
    "\n",
    "* A loop allows information to be passed from one time step of the network to the next.\n",
    "\n",
    "* The output of a unit is dependent on its input and on the previous computations\n",
    "    - RNNs have a memory for the computations performed on earlier time steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Architectures & Applications\n",
    "\n",
    "![](RNN_Configs.png)\n",
    "$$\\text{Figure 2. RNN Architectures}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-to-many: Picture captions\n",
    "    - Input: Picture\n",
    "    - Output: Words in captions\n",
    "* Many-to-one: Sentiment Analysis\n",
    "    - Input: Sentence(s)\n",
    "    - Output: Sentiment score (e.g. 90% Positive)\n",
    "* Many-to-many: Translation\n",
    "    - Input: Source language sentence\n",
    "    - Output: Target language sentence\n",
    "* Many-to-many (synced): Video Classification\n",
    "    - Input: Movie frame\n",
    "    - Output: Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN layer\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtjZu2iAp6OM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# N = number of samples\n",
    "# T = sequence length\n",
    "# D = number of input features\n",
    "# M = number of hidden units\n",
    "# K = number of output units\n",
    "\n",
    "\n",
    "N = 1\n",
    "T = 10\n",
    "D = 3\n",
    "M = 5\n",
    "K = 2\n",
    "X = np.random.randn(N, T, D)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdBewhacp81e"
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "  def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "    super(SimpleRNN, self).__init__()\n",
    "    self.D = n_inputs\n",
    "    self.M = n_hidden\n",
    "    self.K = n_outputs\n",
    "    self.rnn = nn.RNN(\n",
    "        input_size=self.D,\n",
    "        hidden_size=self.M,\n",
    "        nonlinearity='tanh',\n",
    "        batch_first=True)\n",
    "    self.fc = nn.Linear(self.M, self.K)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    # initial hidden states\n",
    "    h0 = torch.zeros(1, X.size(0), self.M)\n",
    "\n",
    "    # get RNN unit output\n",
    "    out, _ = self.rnn(X, h0)\n",
    "    out = self.fc(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPArOn1VqK6p"
   },
   "outputs": [],
   "source": [
    "model = SimpleRNN(n_inputs=D, n_hidden=M, n_outputs=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "cBhtRdsWqesH",
    "outputId": "e6df59a9-5cb7-4521-cb2f-b5fea66106a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3739, 0.6823],\n",
       "         [0.2691, 0.2821],\n",
       "         [0.3581, 0.5325],\n",
       "         [0.2421, 0.5692],\n",
       "         [0.3927, 0.7252],\n",
       "         [0.2474, 0.5856],\n",
       "         [0.1892, 0.6481],\n",
       "         [0.3925, 0.5913],\n",
       "         [0.1869, 0.2363],\n",
       "         [0.3411, 0.2840]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Forward\n",
    "inputs = torch.from_numpy(X.astype(np.float32))\n",
    "out = model(inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZtWi7zpiHytS",
    "outputId": "b21f90a0-ea7b-4e21-8d32-060d2bf15dea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ElomWG3CHM-u"
   },
   "outputs": [],
   "source": [
    "Yhats_torch = out.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hbbkp9rqx-V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 5]), torch.Size([5]), torch.Size([5]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_xh, W_hh, b_xh, b_hh = model.rnn.parameters()\n",
    "W_xh.shape, W_hh.shape, b_xh.shape, b_hh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n5I00hv6I3Gf",
    "outputId": "f834d8e0-95cf-4835-ec21-0831cda05a8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_xh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "4DHhFRFQrLE4",
    "outputId": "9fd10d70-9395-4612-9c9f-833b60efa0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2622, -0.1990, -0.0104],\n",
       "        [ 0.1048, -0.2302,  0.3343],\n",
       "        [ 0.4376,  0.1810, -0.0932],\n",
       "        [ 0.1215, -0.1830,  0.4030],\n",
       "        [ 0.1614,  0.2495, -0.2871]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(W_xh))\n",
    "W_xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "484rEtjsrNFh",
    "outputId": "817242be-df08-4c72-bddd-566eca362daf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26219094, -0.19896993, -0.01044697],\n",
       "       [ 0.10477024, -0.23018256,  0.3342694 ],\n",
       "       [ 0.4376182 ,  0.18097007, -0.093178  ],\n",
       "       [ 0.1214689 , -0.18295664,  0.40295953],\n",
       "       [ 0.1613673 ,  0.24948424, -0.2871253 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_xh = W_xh.data.numpy()\n",
    "W_xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4kmrm-SOrvIf",
    "outputId": "9ed3ae71-4e6c-4f2e-de1c-03f5e8e8640c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3), (5,), (5, 5), (5,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_xh = b_xh.data.numpy()\n",
    "W_hh = W_hh.data.numpy()\n",
    "b_hh = b_hh.data.numpy()\n",
    "\n",
    "W_xh.shape, b_xh.shape, W_hh.shape, b_hh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGagznQWsaZy"
   },
   "outputs": [],
   "source": [
    "\n",
    "Wo, bo = model.fc.parameters() # FC layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dnlLdwjfsfNa",
    "outputId": "a358cd2a-dc81-4543-cfc1-820b2d24f09a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 5), (2,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wo = Wo.data.numpy()\n",
    "bo = bo.data.numpy()\n",
    "Wo.shape, bo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "PUsPqqO4sA3A",
    "outputId": "81a72b08-c5d2-4f51-e3cd-b1a28786f4b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20528887 -0.79961925]\n",
      " [-0.15380444 -0.44783627]\n",
      " [ 0.3585217  -0.71043858]\n",
      " [-0.13168303 -0.52131308]\n",
      " [ 0.31756821 -0.24142816]\n",
      " [ 0.03747482 -0.67827091]\n",
      " [ 0.07696692 -0.52061347]\n",
      " [ 0.18046382 -0.27259799]\n",
      " [ 0.37582838 -0.95786155]\n",
      " [-0.02171963 -0.37523845]]\n"
     ]
    }
   ],
   "source": [
    "# Replicate the output\n",
    "h_last = np.zeros(M) # initial hidden state\n",
    "x = X[0] \n",
    "Yhats = np.zeros((T, K)) \n",
    "\n",
    "for t in range(T):\n",
    "  h = np.tanh(x[t].dot(W_xh.T) + b_xh + h_last.dot(W_hh.T) + b_hh)\n",
    "  y = h.dot(Wo.T) + bo \n",
    "  Yhats[t] = y\n",
    "  h_last = h\n",
    "\n",
    "Yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q-Bz_OjvHE5a",
    "outputId": "8f91d889-74ba-4eba-c043-dde9abb41af7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(Yhats, Yhats_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5,), (5, 2), (2,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape,Wo.T.shape,bo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation Through Time (BPTT)\n",
    "\n",
    "#### Examples using autocompletion\n",
    "\n",
    "![](RNN2.png)\n",
    "![](BPTT1.png)\n",
    "\n",
    "$$\\text{Figure 3. Autocompletion examples}$$\n",
    "\n",
    "#### Parameter $\\theta$ dimensions  \n",
    "\n",
    "Inputs: $x_i \\in R^n$  \n",
    "States: $s_i \\in R^d$  \n",
    "Classes: $y_i \\in R^k$  \n",
    "Input weights: $U_i \\in R^{n x d}$  \n",
    "Hidden weights: $W_i \\in R^{d x k}$  \n",
    "Output weights:$V_i \\in R^{d x d}$  \n",
    "\n",
    "#### Loss function: L\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$ L(\\theta) = \\sum_{t = 1}^T {L_t(\\theta)}$$\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Compute gradient of  $L(\\theta)$ wrt  =  W,U,V\n",
    "\n",
    "* Derivation of Loss wrt weights in the input and output layer:\n",
    "\n",
    "$$\\frac{\\partial L(\\theta)}{\\partial U} =  \\sum_{t = 1}^T \\frac{\\partial L_t(\\theta)}{\\partial U}$$\n",
    "\n",
    "$$\\frac{\\partial L(\\theta)}{\\partial V} =  \\sum_{t = 1}^T \\frac{\\partial L_t(\\theta)}{\\partial V}$$\n",
    "\n",
    "* Derivation of Loss wrt weights in hidden layer for each previous time step\n",
    "\n",
    "![](BPTT3.png)\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\frac{\\partial L(\\theta)}{\\partial W} =  \\sum_{t = 1}^T \\frac{\\partial L_t(\\theta)}{\\partial W}$$\n",
    "\n",
    "$$\\frac{\\partial L_t(\\theta)}{\\partial W} = \\frac{\\partial L_t(\\theta)}{\\partial s_i} \\sum_{k=1}^t\\frac{s_t}{s_k}\\frac{\\partial^+s_k}{\\partial W}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing and Exploding Gradient\n",
    "\n",
    "* Gradient can vanish (i.e. goto zero) or explode \n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\frac{s_t}{s_k} = \\prod_{j = k}^{t - 1}\\frac{\\partial s_{j+1}}{\\partial s_j}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "* Sepp Hochreiter & Jurgen Schmidhuber (1997)\n",
    "\n",
    "* Long-range dependencies\n",
    "    - Language Processing\n",
    "    \n",
    "* Does not suffer from vanishing gradient\n",
    "\n",
    "![](LSTM1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell State (C) and Gates\n",
    "\n",
    "* Cell state is a vector of cell states which runs entire length of sequence\n",
    "* Add and delete information from the cell state through **gates** \n",
    "* **gates** are the circles with the elementwise operations of x and a sigmoid activation function\n",
    "    - The sigmoid outputs a value between 0 and 1 which controls how much information to let through\n",
    "    - There are 3 gates in the figure\n",
    "        - Input \n",
    "        - Forget\n",
    "        - Output\n",
    "    \n",
    "![](LSTM2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### LSTM Operation\n",
    "\n",
    "#### Forget gate layer step  \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "    \n",
    "$$f_t = \\sigma(W_f\\centerdot [h_{t-1},x_t] + b_f)$$\n",
    "\n",
    "</div>\n",
    "\n",
    "* Inputs the output of the previous timestep $h_{t-1}$ and the current input $x_t$\n",
    "    - Dotted with a weight matrix and input to the sigmoid\n",
    "* The sigmoid outputs a vector (matching the cell state), with values ranging from 0 to 1.\n",
    "    - Its a probability of how much of the corresponding value in cell state to keep\n",
    "* Modifies the cell state vector by elementwise multiplication.\n",
    "\n",
    "#### Determine what to store in cell state step (3 steps)  \n",
    "\n",
    "#### 1) input gate layer\n",
    "\n",
    "* Regulates (filters) what values from $h_{t-1}$ and $x_t$ need to be added to the cell state. \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$i_t = \\sigma(W_i\\centerdot[h_{t-1},x_t] + b_i)$$\n",
    "</div>\n",
    "\n",
    "#### 2) Create candidate update $\\hat{C}_t$\n",
    "\n",
    "* Creates a vector containing the possible values that can be added to the cell state by the tanh function, which outputs values from -1 to +1.  \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$\\hat{C}_t = tanh(W_C\\centerdot[h_{t-1},x_t] + b_C)$$\n",
    "</div>\n",
    "\n",
    "#### 3) Update the Cell State step\n",
    "\n",
    "* Multiplies the value of the filter (i.e. the sigmoid gate) and the created vector (i.e.the tanh function)\n",
    "* Adds this information to the cell state by addition.\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$C_t = f_t * C_{t-1} + i_i * \\hat{C}_t$$\n",
    "</div>\n",
    "\n",
    "* Multiply old state by $f_t$ (to  forget)\n",
    "* Add new states weighted by how much to update each state\n",
    "\n",
    "#### Output Gate (3 step process)\n",
    "\n",
    "* Not all information in the cell state, is \"fit\" for being output at this time step\n",
    "\n",
    "1) Sigmoid layer decides what part of cell state to output. It filters $h_{t-1}$ and $x_t$ vis a sigmoid function to regulate the values that need to be output from the vector created below in step 2. \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ o_t = \\sigma(W_o\\centerdot[h_{t-1},x_t] + b_0)$$\n",
    "</div>\n",
    "\n",
    "2) Scales the values in the cell state with a tanh function creating a vector $tanh(C_t)$.\n",
    " \n",
    "3) Multiplies the vectors created in steps 1 and 2 and sends it as an output and to the hidden state of the next cell.\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ h_t = o_t * tanh(C_t)$$\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Units (GRUs)\n",
    "\n",
    "* Kyunghyun Cho: Gated Recurrence Unit (GRU) (2014)\n",
    "* Simpler version of LSTM\n",
    "    - No cell state\n",
    "    - 2 gates\n",
    "* Performance comparable to LSTM\n",
    "\n",
    "![](GRU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Gate\n",
    "\n",
    "* The update gate determines how much of the information from previous time steps needs to be passed along.\n",
    "\n",
    "* Add weighted input x and previous h and pass through a sigmoid to update hidden state\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$z_t = \\sigma(W^{(z)}x_t + U^{(z)}h_{t-1}$$\n",
    "</div>\n",
    "\n",
    "#### Reset Gate\n",
    "\n",
    "* Determines how much of the previous information to forget\n",
    "\n",
    "<div style=\"font-size: 125%;\">    \n",
    "$$r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{t-1}$$\n",
    "</div>\n",
    "\n",
    "#### Current Memory Content\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ h_t = tanh(Wx_t + r_t \\odot{Uh_{t-1}})$$\n",
    "</div>\n",
    "\n",
    "* Weighted input + elementwise multiplication of reset gate with weighted previous hidden state\n",
    "* Determines what to remove from previous time steps\n",
    "\n",
    "#### Final Memory Content\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$h_t = z_t\\odot{h_{t-1}}+(1-z_t)\\odot{h_t}$$\n",
    "</div>\n",
    "\n",
    "* Information for current output and to be passed to next time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
