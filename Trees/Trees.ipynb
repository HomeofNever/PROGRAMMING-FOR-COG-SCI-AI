{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports for trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based methods for Regression and Classification\n",
    "\n",
    "* Non-parametric supervised learning methods for classification and regression.\n",
    "* These methods learn simple decision rules inferred from the training data to predict target values\n",
    "* Tree methods segment the feature space into regions based on Decision (Splitting) rules \n",
    "    - Trees are a representation of these rules \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = pd.read_csv(\"Hitters.csv\")\n",
    "Hitters = Hitters.dropna()\n",
    "Hitters = Hitters.drop(['League','Division','NewLeague'],axis = 1)\n",
    "Hitters['LogSalary'] = np.log(Hitters.Salary)\n",
    "Hitters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree\n",
    "\n",
    "* Sequence of binary decisions\n",
    "* Build Tree based on Training data\n",
    "    * Decide which feature and feature value to split on\n",
    "    * Root node: the top at the top of the tree\n",
    "    * Leaf (terminal) node: Lowest node, contains value or class of target variable\n",
    "\n",
    "\n",
    "\n",
    "![](simpleDecisionTree1.png)\n",
    "$$\\text{Figure 1. Simple Decision Tree}$$\n",
    "\n",
    "1. Split on Years < 4.5\n",
    "2. Split on Hits < 117.5\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divides the space into regions ($R_1, R_2, R_3, ...$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'Years',y='Hits',hue='LogSalary',data = Hitters)\n",
    "plt.plot([4.5,4.5],[0,250])\n",
    "plt.plot([4.5,25],[117.5,117.5])\n",
    "plt.plot(3,100,color = 'black',marker = 's');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "* Predict new observation by transversing the tree from the top according to the features and feature values of the new data point.\n",
    "\n",
    "* For new point (black square) assign value or class based on a metric of other points in the region (i.e. node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "\n",
    "* CART (Breiman et. al., 1984): Classification and Regression Trees\n",
    "    - CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n",
    "    * sklearn uses an optimized version of CART except **Independent variables can not be categorical** at this time\n",
    "* Quinlan: ID3(1986), C4.5(1993), C5.0 \n",
    "    - only Classification Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Binary Splitting (a top-down greedy algorithm)\n",
    "\n",
    "* Given feature vector $X \\in R^p$, it is computationally infeasible to consider every possible partition of the feature space into regions.  \n",
    "\n",
    "![](Regions_tree.png)\n",
    "$$\\text{Figure 2. Regions and Tree produced by Recursive Bianry Splitting}$$\n",
    "\n",
    "* Top-down: begins at the top of the tree and then successively splits the predictor space\n",
    "* Greedy: at each step of the tree-building process, the best split is made at that particular step\n",
    "    - No look ahead to pick a split that will lead to a better tree in some future step.\n",
    "* Select the predictor $X_j$ and the cutpoints s such that splitting the space into the regions {X|$X_j$ < s} and {X|$X_j$ >  s} leads to the greatest possible reduction in some Regression or Classification metric\n",
    "* Repeat the process on the resulting regions\n",
    "    - Find the best $X_j$ and s to further split the data minimizing the Regression or Classification metric within each of the resulting regions. \n",
    "* Continue the process until a stopping criterion is reached\n",
    "* Can use a feature multiple times with different values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "* Data: $(x_1,y_1),(x_2,y_2),...(x_n,y_n)$, $x_i \\in R^d$, $y_i \\in R$\n",
    "    - d features\n",
    " \n",
    "* Goal: Find regions $R_1,R_2,...,R_j$ to minimize Mean Squared Error (MSE) or Mean Absolute Error (MAE)\n",
    "    - Which of the d features to split on and which value of the feature?\n",
    "\n",
    "#### Regression Prediction\n",
    "\n",
    "* Response of new observation is the mean of the training observations in the region to which the new observation belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics\n",
    "\n",
    "* Given Node m representing a region $R_m$ with $N_m$ observations ($x_i,y_i)$  \n",
    "* The mean response for the training observations within the ith region is:\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{N_m}\\sum_{i\\in{N_m}}y_i$$\n",
    "\n",
    "* $X_m$ is the training data in node m\n",
    "\n",
    "#### $H(X_m)$ is called the impurity function.\n",
    "* We want nodes to be pure\n",
    "\n",
    "#### Mean Squared Error\n",
    "\n",
    "* Minimizes L2 Error \n",
    "\n",
    "$$H(X_m) = \\frac{1}{N_m}\\sum_{i \\in {N_m}}(y_i - \\bar{y}_m)^2 $$\n",
    "\n",
    "#### Mean Absolute Error\n",
    "\n",
    "* Minimizes L1 Error \n",
    "\n",
    "$$H(X_m) = \\frac{1}{N_m}\\sum_{i \\in {N_m}}|y_i - \\bar{y}_m| $$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Overfitting   \n",
    "\n",
    "* Decision Tree learning tends to make overly complex models which overfit\n",
    "* Reduce size of tree since a smaller tree will have less variance \n",
    "    - Limit the depth of the tree\n",
    "    - Set the minimum number of samples required at a leaf node\n",
    "    - sklearn uses Minimal Cost-Complexity (see Appendix)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Regression Trees in sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "* criterion - the function to measure the quality of the split\n",
    "    - 'mse'\n",
    "    - 'friedman_mse'\n",
    "    - 'mae'\n",
    "\n",
    "* max_depth: control size by limiting tree depth\n",
    "* min_samples_split: minimum # of samples required to split an internal node\n",
    "* min_samples_leaf: control size by setting minimum number of samples at leaf nodes\n",
    "* random_state: To obtain a deterministic behavior during fitting, set random_state.\n",
    "* ccp_alpha: cost-complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feats = ['Years','Hits']\n",
    "feats = Hitters.columns.tolist()[:-2] # All the features\n",
    "X = Hitters.loc[:,feats].values\n",
    "y = Hitters.loc[:,'Salary'].values\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Tree to Training data\n",
    "dt = DecisionTreeRegressor(random_state = 1234)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Test data\n",
    "y_pred = dt.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test)**2)\n",
    "print(\"Test error is: \",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = dt.score(X_test,y_test);R2\n",
    "print(f'R-squared = {np.round(R2,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vals = dt.feature_importances_\n",
    "pairs = [(vals[i],feats[i]) for i in range(len(vals))]\n",
    "pairs.sort(reverse=True)\n",
    "for val,feat in pairs:\n",
    " print(feat,'\\t',round(val,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(dt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(dt, X,y, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores))\n",
    "plt.plot(range(1,11),scores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                         feature_names=feats,  \n",
    "                         class_names=['Salary'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install graphviz with conda or pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install graphviz\n",
    "# conda install graphviz\n",
    "# conda install python-graphviz\n",
    "\n",
    "import graphviz\n",
    "graph = graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.render(\"Hitters\",view = True,cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save text dot file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotfile = open(\"Hitters_tree.dot\", 'w')\n",
    "tree.export_graphviz(dt, out_file=dotfile, feature_names=feats)\n",
    "dotfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    " \n",
    "* Goal:  \n",
    "    - Predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\n",
    "    \n",
    "* Classes should be balanced to avoid  creating biased trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Predictions\n",
    "\n",
    "* Response of new observation is the class the has the high proportion of observations in the region to which the new observation belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics \n",
    "\n",
    "* Given Node m representing a region $R_m$ with $N_m$ observations ($x_i,y_i)$  \n",
    "* The proportion if class k observations in node m is:\n",
    "\n",
    "$$ p_{mk} = \\frac{1}{N_m}\\sum_{x_i \\in R_m}I(y_i ==k)$$\n",
    "\n",
    "* $X_m$ is the training data for node m\n",
    "\n",
    "#### Gini impurity\n",
    "\n",
    "\n",
    "    \n",
    "$$H(X_m) = \\sum_{k=1}^Kp_{mk}(1-p_{mk})$$\n",
    " \n",
    "* where $p_{mk}$ is the proportion of training observations in the mth region that are from the kth class.\n",
    "    - Takes on a small value if all of the $p_{mk}$s are close to\n",
    "zero or one.\n",
    "    - Node purity: a small value indicates that a node contains predominantly observations from a single class.\n",
    " \n",
    "#### Entropy\n",
    "\n",
    "* See Appendix\n",
    " \n",
    "$$H(X_m) = -\\sum_{k=1}^Kp_{mk}log(p_{mk})$$\n",
    "\n",
    "* In most cases the Gini and Entropy metrics produce the same results\n",
    "\n",
    "#### Misclassification\n",
    "\n",
    "$$H(X_m) = 1 - max(p_{mk})$$\n",
    "\n",
    "* Not widely used because not sensitive enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Classification Trees in sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "* criterion - the function to measure the quality of the split\n",
    "    - 'gini'\n",
    "    - 'entropy'\n",
    "\n",
    "* max_depth: control size by limiting tree depth\n",
    "* min_samples_split: minimum # of samples required to split an internal node\n",
    "* min_samples_leaf: control size by setting minimum number of samples at leaf nodes\n",
    "* random_state: To obtain a deterministic behavior during fitting, set random_state.\n",
    "* ccp_alpha: cost-complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wine = pd.read_csv(\"Wine.csv\")\n",
    "feats = Wine.columns[0:-1]\n",
    "print(len(feats))\n",
    "Wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Wine.iloc[:,0:13].values\n",
    "y = Wine.iloc[:,13].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify = y, test_size = 0.25, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decision tree classifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Test data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Make  the Confusion Matrix and calculate  model accuracy\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy: \",round(np.trace(cm)/np.sum(cm),3))\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "np.random.seed(42)\n",
    "scores = cross_val_score(dt, X,y, cv=10, scoring = 'accuracy')\n",
    "print(scores)\n",
    "print(\"Mean Accuracy: \",round(np.mean(scores),3))\n",
    "plt.plot(range(1,11),scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                         feature_names= feats,  \n",
    "                         class_names= Wine.columns[-1],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install graphviz\n",
    "# conda install graphviz\n",
    "# conda install python-graphviz\n",
    "\n",
    "import graphviz\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render(\"Wine\",view = True,cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotfile = open(\"Wine_tree.dot\", 'w')\n",
    "tree.export_graphviz(dt, out_file=dotfile, feature_names=feats)\n",
    "dotfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "\n",
    "* Simple and interpretable. \n",
    "* Trees can be visualized  \n",
    "* Requires little data preparation (e.g. no scaling)\n",
    "* Rules for numerical, ordinal and categorical data\n",
    "* Mirror human decision-making??   \n",
    " \n",
    "\n",
    "### Cons\n",
    " \n",
    "* Tends to overfit. Creates overly complex trees that do not generalize well. \n",
    "    - Need to prune trees \n",
    "* Highly variable. Small change in data can produce completely different tree\n",
    "* Finding optimum tree is NP-Complete.\n",
    "    - Use Greedy algorithm which may not produce the best tree \n",
    "* Prediction accuracy is not competitive with other approaches\n",
    "    - But with Ensemble Methods (Bagging, Random Forests and Boosting) can produce results that surpass other common methods in use today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pruning decision trees with cost complexity pruning (CCP)\n",
    "\n",
    "* Cost Complexity Pruning controls the size of a tree to alleviate overfitting\n",
    "\n",
    "* The keyword ccp_alpha is the CCP parameter.\n",
    "    - Greater values increase number of nodes pruned\n",
    "    \n",
    "* The best value of ccp_alpha can be determined from accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total impurity of leaves and alphas of pruned tree\n",
    "\n",
    "* Minimal cost complexity pruning recursively finds the node with the \"weakest\n",
    "link\". The weakest link is characterized by an effective alpha, where the\n",
    "nodes with the smallest alpha are pruned first. \n",
    "\n",
    "* scikit-learn provides DecisionTreeClassifier.cost_complexity_pruning_path to \n",
    "determine the best  ccp_alpha.\n",
    "    - It returns the alphas and the corresponding total leaf impurities at each step of\n",
    "the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer(as_frame=True)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['frame']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data'].values\n",
    "y = data['target'].values\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.25,random_state=0,stratify=y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "print(path.keys())\n",
    "print(ccp_alphas.shape,impurities.shape)\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The maximum effective alpha value is removed, because\n",
    "it is the trivial tree with only one node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a decision tree using the effective alphas. \n",
    "\n",
    "* The last value in ccp_alphas is the alpha value that prunes the whole tree, leaving the last tree with one node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    model = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "print(f\"Number of nodes in the last tree is: {models[-1].tree_.node_count} with ccp_alpha: {ccp_alphas[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Show that the number of nodes and tree depth decreases as alpha increases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models[:-1] # Remove lat element because it is the trivial tree with only one node\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [m.tree_.node_count for m in models]\n",
    "depth = [m.tree_.max_depth for m in models]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs alpha for training and testing sets\n",
    "\n",
    "* When ccp_alpha is set to zero and the otherparameters are set to their default values\n",
    "the tree overfits\n",
    "    - 100% training accuracy and 90% testing accuracy. \n",
    "* As alpha increases, more of the tree is pruned which will generalize better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [m.score(X_train, y_train) for m in models]\n",
    "test_scores = [m.score(X_test, y_test) for m in models]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "print(f'The best alpha is {ccp_alphas[np.argmax(test_scores)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pruning Decision Tree Regression\n",
    "\n",
    "1) Determine the best ccp_alpha for the seaborn 'mpg' dataset.\n",
    "\n",
    "2) Fit a Decision Tree Regressor to the seaborn 'mpg' dataset. Predict tip as a  function of all the features.  Use the ccp_alpha from 1)\n",
    "\n",
    "3) Output feature importances and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sns.load_dataset('mpg')\n",
    "mpg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.dropna(inplace=True)\n",
    "np.sum(mpg.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg.iloc[:,1:-2].values\n",
    "y = mpg.loc[:,'mpg'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.20, random_state = 1234)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Entropy\n",
    " \n",
    "#### Information: The reduction in uncertainty derived from learning an outcome\n",
    "\n",
    "* Quantified by  minus the log probability of an event\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ I = -log_2P(x)$$\n",
    "</div>\n",
    "\n",
    "* x is an event, e.g. a coin flip came up heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose P(X=heads) = 1/2, P(X = tails) = 1/2\n",
    "from math import log\n",
    "\n",
    "I = -log(1/2,2) #Base 2\n",
    "print(\"{} bit of information\".format(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose P(X=heads) = 3/4, P(X = tails) = 1/4\n",
    "I = -log(3/4,2)\n",
    "print(\"{} bits of information\".format(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(x) = 3/4 > P(x) = 1/2, but I = -log(1/2,2) < I = -log(3/4,2)\n",
    "\n",
    "Probability $\\uparrow$ Information $\\downarrow$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Entropy is the Average Information\n",
    "\n",
    "* Minus the Expected Value of Information\n",
    "\n",
    "* Suppose you have a 4-sided die with the probability of tossing a 1 = 1/2, a 2 = 1/4, 3 = 1/8, 4 =  1/8\n",
    "    - i.e. pmf = (1/2,1/4,1/8,1/8)\n",
    "    - E(X) = $\\sum_ixp(x)$ = 1(1/2)+2(1/4)+3(1/8)+4(1/8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected value of X is: \", 1*(1/2)+2*(1/4)+3*(1/8)+4*(1/8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 115%;\">\n",
    "$$ Entropy = H = -E(I) = -\\sum_i p(x_i)*log(p(x_i))$$\n",
    "</div>\n",
    "* $p(x_i)$ is a PMF\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entropy is: \",-1*(1/2*log(1/2,2) + 1/4*log(1/4,2) + 1/8*log(1/8,2) + 1/8*log(1/8,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differential Entropy - Continuous RV\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$Entropy = H = -\\int{p(x)*log(p(x)} dx$$\n",
    "</div>\n",
    "\n",
    "* $p(x)$ is a PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal Cost-Complexity Pruning \n",
    "\n",
    "* Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "\n",
    "* Minimal cost complexity pruning recursively finds the node with the \"weakest\n",
    "link\" where the weakest link is characterized by the complexity parameter $\\alpha \\ge 0$ \n",
    "    - The nodes with the smallest alpha are pruned first. \n",
    "    \n",
    "#### Cost-complexity measure $R_{\\alpha}(T)$ of a given tree T\n",
    "\n",
    "$$R_{\\alpha}(T) = R(T) + \\alpha|T|$$\n",
    "\n",
    "|T|: number of terminal nodes in T  \n",
    "R(T): total sample impurity of the terminal nodes in T  \n",
    "\n",
    "*  Minimal cost-complexity pruning finds the subtree of T that minimizes $R_{\\alpha}(T)$\n",
    "    \n",
    "#### Effective $\\alpha$\n",
    "\n",
    "* The cost complexity measure of a single node is $R_{\\alpha}(t) = R(t) + \\alpha$\n",
    "* The branch $T_t$ is defined to be a tree where node t is the root node.\n",
    "* The **effective $\\alpha$** of a node t is when  $R_{\\alpha}(T_t) = R_{\\alpha}(t)$ i.e. when cost complexity measure of a node, t , and its branch $T_t$ are equal\n",
    "$$ \\alpha_{eff} = \\frac{R(t) - R(T_t)}{|T|-1}$$\n",
    "\n",
    "* A non-terminal node with the smallest value of $\\alpha_{eff}$ is the weakest link and will be pruned. \n",
    "* Pruning stops when the tree’s minimal $\\alpha_{eff}$ is greater than the ccp_alpha parameter in sklearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures 1 and 2 are taken from \"An Introduction to Statistical Learning, with applications in R\" (Springer, 2013) with permission from the authors: G. James, D. Witten, T. Hastie and R. Tibshirani \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "302px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
