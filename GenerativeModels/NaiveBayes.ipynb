{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Discriminative and Generative Models\n",
    "\n",
    "#### Discriminative Models\n",
    "\n",
    "* Logistic regression, multivariate logistic and softmax regression are discriminative models\n",
    "* They all directly compute the conditional probability P(y|x), i.e. the probability of a given class knowing the feature x\n",
    "\n",
    "* Models the boundary between classes\n",
    "    - Classify points without providing a model of how the points are actually generated\n",
    "    - Discriminative models directly model the mapping from the independent variables to the dependent ones\n",
    "    - Classification is done using a threshold to turn the computed probability into a boundary that determines class assignment\n",
    "* Doesn't make as many assumptions\n",
    "\n",
    "#### Generative Models\n",
    "\n",
    "* Naive Bayes and Linear Discriminant Analysis (LDA) are Generative Models\n",
    "* First model P(x|y) i.e. model the distribution of x for each class and then assign the classes\n",
    "* Computes joint distribution: P(x,y) = P(x|y)P(y)\n",
    "* Enables generation of samples for each class\n",
    "* Models how the data was generated\n",
    "* Make assumptions about structure of data\n",
    "    - Independence assumption in Naive Bayes\n",
    "    - The distribution of the classes (P(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "* A family of classification algorithms based on Bayes' Theorem\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$P(Class|Features) = \\frac{P(Class)P(Features|Class)}{P(Features)}$$\n",
    "</div>\n",
    "\n",
    "* They all assume that the features (i.e. predictors) are independent, the \"Naive\" assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf = pd.read_csv(\"golf.csv\",quotechar=\"'\",escapechar=\"/\")\n",
    "golf.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golf dataset\n",
    "\n",
    "* Feature Matrix\n",
    "    - 14 observations of 4 features: forecast,temp, humidity, and wind \n",
    "* Response Vector\n",
    "    - Contains value of class variable\n",
    "    - Two classes or categories: Play=yes and Play = no.\n",
    "* Goal is to predict whether to play golf given a set of weather conditions.\n",
    "    - For today's data, D = (forecast,temp,humidity and wind), which is more probable, P(Play = yes|D) or P(Play = no|D)\n",
    "* Assume features are independent and are equally important in predicting Play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Probabilities P(C)\n",
    "\n",
    "* Count number of 'yes's and 'No's and \n",
    "* Divide by total number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = golf.Play.value_counts()\n",
    "print(cnts)\n",
    "L = np.sum(cnts)\n",
    "\n",
    "P_yes = cnts['Yes']/L\n",
    "P_no = cnts['No']/L\n",
    "P_yes.round(2),P_no.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Feature Probabilities P(feature = value | Class = value)\n",
    "\n",
    "* The likelihood of the feature given the class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_feat_prob (feat,val,cls='Yes'):\n",
    "    return((sum((golf[feat] == val) & (golf.Play == cls))/cnts[cls]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Humudity\n",
    "P_normal_yes = cond_feat_prob('humidity','Normal')\n",
    "P_high_yes = cond_feat_prob('humidity','High')\n",
    "P_normal_no = cond_feat_prob('humidity','Normal',cls='No')\n",
    "P_high_no = cond_feat_prob('humidity','High',cls='No')\n",
    "P_normal_yes,P_high_yes,P_normal_no,P_high_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind\n",
    "P_weak_yes = cond_feat_prob('wind','Weak')\n",
    "P_strong_yes = cond_feat_prob('wind','Strong')\n",
    "P_weak_no = cond_feat_prob('wind','Weak',cls='No')\n",
    "P_strong_no = cond_feat_prob('wind','Strong',cls='No')\n",
    "P_weak_yes,P_strong_yes,P_weak_no,P_strong_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Play today = yes  given today's  features:  P(Play=yes|X) ~ P(X|Play=yes) * P(Play=yes)\n",
    "\n",
    "* Probability that you **will** play today is proportional to the Likelihood of the features times the Prior probability that you will play based on your past experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = ('High','Strong') # Today has High Humidity and Strong Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_yes_today = (P_high_yes * P_strong_yes) * P_yes # Independence Assumption\n",
    "P_yes_today.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Play today = no  given today's  features: P(Play=no | X) ~ P(X|Play=no) * P(Play = no)\n",
    "\n",
    "* Probability that you **wont** play today is proportional to the Likelihood of the features times the Prior probability that you will play based on your past experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_no_today = (P_high_no * P_strong_no) * P_no #Independence Assumption\n",
    "P_no_today.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play?\n",
    "\n",
    "* Compare P_yes_features to P_no_features\n",
    "    - Don't need to compute P(Features) to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Play') if (P_yes_today / P_no_today) > 1 else print(\"Study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to probabilities by normalizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_today = P_yes_today + P_no_today \n",
    "\n",
    "P_yes_X = P_yes_today / P_today\n",
    "P_no_X = P_no_today / P_today\n",
    "\n",
    "P_yes_X,P_no_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier\n",
    " \n",
    "\n",
    "* A Bayes Classifier assigns to each observation the most likely class given its feature vector. \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ \\arg\\max_{y} Pr(Y = y|X = x)\\text{, }x \\in R^d\\text{, }y\\in \\{1,2,3,..,m\\}$$\n",
    "</div>\n",
    "\n",
    "* Naive Bayes is a special case of a Bayes Classifier since it assumes the features are independent\n",
    "\n",
    "#### Test Error Rate for Classification:\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\frac{1}{n}\\sum^n_{i=1}I(y_i \\neq \\hat{y}_i)$$  \n",
    "</div>\n",
    "\n",
    "$I(y_i \\neq \\hat{y}_i)$ is an indicator variable = 1 if $y_i \\neq \\hat{y}_i$ and 0 if $y_i = \\hat{y}_i$\n",
    "\n",
    "* It can be proven that the Bayes Classifier produces the lowest possible test error rate, called the Bayes Error Rate\n",
    "\n",
    "<div style=\"font-size: 105%;\">\n",
    "$$ BayesErrorRate = 1 - E(\\arg\\max_{y}(Pr(Y=y|X)))$$\n",
    "</dev>\n",
    "\n",
    "* The Expectation (E) averages the probability over all possible values of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### How to calculate P(Y=Class|X = features) ??\n",
    "\n",
    "* In general you can't compute the posterior conditional distribution because you have on the order of $2^n$ parameters if you have n binary features\n",
    "    \n",
    "* Therefore you need to make some assumption.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "#### Naive Bayes Assumptions:\n",
    " \n",
    "* All features are of equal importance \n",
    "* All features are conditionally independent\n",
    "    - This is often violated, since the features are often somewhat correlated, \n",
    "    - Even so, it works surprisingly well in many cases.\n",
    "    - Reduces the number of parameters (on the order of n parameters)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Probability\n",
    " \n",
    "* What is the probability that a particular object belongs to class y given its observed feature values?\n",
    "*  Given:\n",
    "    - y is the class variable \n",
    "    - $x_i$ represents ith feature in the feature vector \n",
    "\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "\n",
    "$$P(y|x_1,...,x_d) = \\frac{P(y)P(x_1,...,x_d | y)}{P(x_1,...,x_d)}$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence Assumption\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ P(x_i|y,x_1,...,x_{i-1},x_{i+1},...,x_d) = P(x_i|y)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Independence Assumption\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$P(y|x_1,...,x_d) = \\frac{P(y)\\prod^d_{i=1}P(x_i|y)}{P(x_1,...,x_d)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $P(x_1,...,x_d)$ doesn't depend on y (it is a constant), so\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$P(y|x_1,...,x_d) \\propto{P(y)\\prod^d_{i=1}P(x_i|y)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Rule\n",
    " \n",
    "* Maximize the posterior probability given the training data to formulate a decision rule for new data.\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$\\hat{y} = \\arg\\max_{y} P(y)\\prod^d_{i=1}P(x_i|y)$$\n",
    "</div>\n",
    "\n",
    "#### Estimate the prior P(y) and the Likelihood $P(x_i|y)$ from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Prior Probability P(y)\n",
    " \n",
    "* Probability that new observation belongs to class y.\n",
    "* Expert knowledge or estimated from training data (assumes traing data is i.d.d.)\n",
    "* Maximum Likelihood Estimate:\n",
    "    - The relative frequency of class y in the training data\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ \\hat{P}(Y = y) = \\frac{NumY}{NumTotal}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the  Likelihoods $P(x_i|y)$\n",
    "\n",
    "* Because of the independence assumption, each feature ($x_i$) can have its own distribution\n",
    "    - The feature can be Categorical or Numerical\n",
    "\n",
    "### Categorical data\n",
    "* The likelihoods for every feature is estimated to be simply the frequency (categorical data).\n",
    "* Maximum Likelihood Estimate:\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\hat{P}(X_i = x|Y=y) = \\frac{NumJoint(x,y)}{NumTotal(Y=y)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian numerical data\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\hat{P}(x_i|y) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Gaussian Naive Bayes\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "df = pd.read_csv('KNN_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, [0, 1]].values\n",
    "y = df.iloc[:, 2].values\n",
    "X[0:5,:],y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,\n",
    "                                                    random_state = 1234,stratify= y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Age and Salary features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Naive Bayes model to the training data\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "preds = model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make  the Confusion Matrix and compute the accuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(cm)\n",
    "accuracy = np.trace(cm)/np.sum(cm)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap([\"white\", \"black\"])\n",
    "def plot_decision_boundary(model,X_data,y_data,ax):\n",
    "    # Make grid: min to max of 1st column, min to max of 2nd column in small increments\n",
    "    X1,X2 = np.meshgrid(np.arange(X_data[:, 0].min() - 1, X_data[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(X_data[:, 1].min() - 1, X_data[:, 1].max() + 1, step = 0.01))\n",
    "    # Flatten X1 and X2, create an array and transpose to a 2-column array (one for each feature)\n",
    "    v = np.array([X1.ravel(), X2.ravel()]).T\n",
    "    #  Using fitted model, predict the points in v and reshape to a 2-dimensional array\n",
    "    ax.contourf(X1, X2, model.predict(v).reshape(X1.shape),alpha = 0.75,cmap = ListedColormap(('magenta','cyan')))\n",
    "    ax.set_xlim(X1.min(), X1.max())\n",
    "    ax.set_ylim(X2.min(), X2.max())\n",
    "    for i, j in enumerate(np.unique(y_data)): # For each class\n",
    "        ax.scatter(X_data[y_data == j, 0], X_data[y_data == j, 1],\n",
    "                label = j,\n",
    "                c = np.array(ListedColormap(('white', 'black'))(i)).reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize = (10,6))\n",
    "\n",
    "# Visualising the Training set results\n",
    "plot_decision_boundary(model,X_train,y_train,ax1)\n",
    "ax1.set_title('Training Data')\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.set_ylabel('Salary')\n",
    "\n",
    "\n",
    "# Visualising the Test set results\n",
    "plot_decision_boundary(model,X_test,y_test,ax2)\n",
    "ax2.set_title('Test Data')\n",
    "ax2.set_xlabel('Age')\n",
    "ax2.set_ylabel('Salary');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Distributed Data\n",
    "\n",
    "* Classification with discrete features \n",
    "    - e.g. word counts for text classification.  \n",
    "* Multinomial distribution: extends binomial to more than 2 categories   \n",
    "    * Parameter n is the number of trials  \n",
    "    * Parameter p is a vector of probabilities, one per category  \n",
    "        - Estimated by additive smoothing  \n",
    "* Used in Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Smoothing\n",
    "\n",
    "* In testing, it is possible to encounter a feature not seen in training\n",
    "    - The conditional probability for that feature will be 0 making the class conditional probability 0.\n",
    "* Ensure conditional probability not 0 by adding an additional smoothing term\n",
    "    - d is the number of features\n",
    "    \n",
    "<div style=\"font-size: 115%;\">\n",
    "$$\\hat{P}(x_i|y_j) = \\frac{N_{x_i,y_j} + \\alpha}{Ny_j + \\alpha d}\\text{ }\\forall{i} = 1,2,...d$$\n",
    "</div>\n",
    "\n",
    "* $\\alpha$ is a parameter for additive smoothing  \n",
    "    - Laplace Smoothing: $\\alpha$ = 1  \n",
    "    - Lidstone Smoothing: $\\alpha$ < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Multinomial Naive Bayes\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate some data: counts of words in documents\n",
    "\n",
    "* Rows are topics  \n",
    "* Columns are words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X = np.random.randint(5, size=(6, 100)) #5 = counts, 6 = # of observations, 100 = # of features\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "print(X.shape)\n",
    "print(np.unique(X,return_counts=True))\n",
    "X[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha = 1.0) #alpha is the Laplace/Lidstone smoothing parameter, default = Laplace\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict class of features X[2] and probabilities of each class \n",
    "\n",
    "* Truth is topic = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(X[2:3]))\n",
    "\n",
    "model.predict_proba(X[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernouli Distributed Data\n",
    "\n",
    "* Data is distributed according to multivariate Bernoulli distributions\n",
    "    - There may be multiple features but each one is assumed to be a boolean variable. \n",
    "* Used in Natural Language Processing \n",
    "    - Text Classification\n",
    "    \n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ \\hat{P}(x_i|y) = P(i|y)x_i + (1 - P(i|y))(1 - x_i)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Bernoulli Naive Bayes\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X = np.random.randint(2, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "print(X.shape)\n",
    "print(np.unique(X,return_counts=True))\n",
    "X[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB(alpha = 1.0) #alpha is the Laplace/Lidstone smoothing parameter, default = Laplace\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X[2].reshape(1,-1);X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict class of features (X[2] and probabilities of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(X2))\n",
    "model.predict_proba(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Pros and Cons\n",
    "\n",
    "#### Pros\n",
    "\n",
    "* Easy and fast to predict classes of the test set\n",
    "* Can perform better than other algorithms assuming independence\n",
    "* Performs well with categorical input\n",
    "\n",
    "#### Cons\n",
    "\n",
    "* Zero frequency\n",
    "* Not a good estimator of the probabilities\n",
    "* Independence assumption often violated \n",
    "\n",
    "### Why independence assumption?\n",
    "\n",
    "* Even though it is often a poor assumption, it simplifies the model\n",
    "\n",
    "#### A bad simple model often outperforms a better complex one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "505px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "284px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
