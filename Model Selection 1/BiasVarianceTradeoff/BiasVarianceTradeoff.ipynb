{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance tradeoff\n",
    "\n",
    "$$Y = f(X) + \\epsilon \\text{ where } \\epsilon \\sim N(0,\\sigma)$$\n",
    "\n",
    "* The outcome Y is a function of some independent variables X and some error $\\epsilon \\sim N(0,\\sigma_{\\epsilon})$\n",
    "* f(X) is a real-valued function so this is a regression setting\n",
    "* $\\hat{f}(X)$ estimates f(X) (i.e. is a model for f(x))\n",
    "* The expected prediction error at a point x:\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$Err(x) = E[(Y - \\hat{f}(x))^2]$$\n",
    "</div>\n",
    "* This error may be decomposed:\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$Err(x) = (E[\\hat{f}(x)] - f(x))^2 + E[(\\hat{f}(x) - E[\\hat{f}(x)])^2] + \\sigma^2_{\\epsilon} \\\\\n",
    "Err(x) = Bias^2 + Variance + IrreducibleError$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bias**: The difference of the models average estimate from the true value\n",
    "* **Variance**: the average squared difference of the model on a particular training sample and the average over many training samples.\n",
    "\n",
    "* **Irreducible Error**: The error that can't be reduced even if we had a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](bias-and-variance.jpg)\n",
    "$$\\text{Figure 1. Visualize Bias and Variance}$$\n",
    " \n",
    "![](model-complexity.png)\n",
    "$$\\text{Figure 2. Bias and Variance Curves}$$\n",
    "\n",
    "Source:  http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "* To minimize test error need low variance and low bias  \n",
    "* The Variance is the amount $\\hat{f}$ would change if used different training set. \n",
    "    - More **complex** (i.e. more parameters, more flexible) methods have higher variance\n",
    "    - Tuned to the sample, if get a different sample, will get different reducible error\n",
    "* The Bias is the error introduced by modeling a complicated process with a simpler model\n",
    "    - Linear regression assumes a linear relationship but most real world problems are not purely linear\n",
    "    - More complex models have less bias\n",
    "  \n",
    "* The error curve is U-shaped error because as we increase complexity (i.e.flexibility) bias tends to decrease faster than variance increases (lower test MSE) but at some point, increasing complexity (i.e. adding more parameters) has little effect on bias but significantly increases variance.\n",
    "\n",
    "* Where the U-shape curve starts to increase is the point at which **overfitting** occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity\n",
    "\n",
    "* Models with more parameters (degrees of freedom) are more complex\n",
    "* Simpler models are easier to interpret\n",
    "    - Occam's razor:  if we have two or more equivalent explanations for the same phenomenon, we should choose the simpler one.\n",
    "    - But can underfit the data\n",
    "\n",
    "* Complex models are more **flexible** but are less **interpretable** (not good if inference is the goal)\n",
    "* If prediction is the goal, more complex models tend to be more accurate but can overfit. \n",
    "* We want a model that is high on accuracy but is as simple as possible\n",
    "    - \"Things should be as simple as possible but not simpler\" Albert Einstein\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting/Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overfitting: Overfitting occurs when the learning method is finding patterns in the training data caused by random chance (noise) rather than properties of the unknown function f  \n",
    "    - U-shaped curve  \n",
    "    - Small training MSE and large test MSE indicates overfitting  \n",
    "    - Overfitting occurs when **out-of-sample** mean squared error (MSE) increases\n",
    " \n",
    "* Underfitting: When the learning method is not complex enough to accurately capture the\n",
    "relationships between the response variable and the predictors\n",
    "\n",
    "#### Within-sample (i.e. training) versus out-of-sample (i.e. test) accuracy\n",
    "\n",
    "* **Within-sample** accuracy is the accuracy of model fit to the training data. It will always increase with more parameters\n",
    "* **Out-of-sample** accuracy is the accuracy of the model on new data. When it starts to decrease (error starts to increase) is the point at which it is overfitting the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['seaborn-darkgrid'])\n",
    "x_n = np.linspace(4,14, 100)\n",
    "degree = [0, 1, 2, 5]\n",
    "\n",
    "x = np.array([4.,5.,6., 9.,12, 14.])\n",
    "y = np.array([4.2, 6., 6., 9., 10, 10.])\n",
    "\n",
    "def R_squared(y,yhat):\n",
    "    ybar = np.mean(y)\n",
    "    ssreg = np.sum((yhat-ybar)**2)\n",
    "    sstot = np.sum((y - ybar)**2) \n",
    "    return np.round(ssreg / sstot,2)\n",
    "\n",
    "def plot_poly(x,y):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.plot(x, y, 'o')\n",
    "    for i in degree:\n",
    "        coeffs = np.polyfit(x, y, deg=i) # numpy polynomial regression\n",
    "        ffit = np.polyval(coeffs, x_n) \n",
    "        poly = np.poly1d(coeffs) # Construct polynomial with coeffs\n",
    "        yhat = poly(x) # Evaluate\n",
    "        r2 = R_squared(y,yhat)\n",
    "        plt.plot(x_n, ffit, label=f'degree {i}, $R^2$= {r2}')\n",
    "    plt.legend(loc=2, fontsize=14)\n",
    "    plt.xlabel('$x$', fontsize=14)\n",
    "    plt.ylabel('$y$', fontsize=14, rotation=0)\n",
    "\n",
    "plot_poly(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model increases in complexity as more terms (parameters) are added and $R^2$ increases explaining more of the variance in y\n",
    "\n",
    "#### Overfitting\n",
    " \n",
    "* Model with degree 5 fits the data perfectly\n",
    "    - Same number of parameters as data points\n",
    "    - Notice the erratic behavior of this model between the data points (9,9) and (12,9).\n",
    "* The behavior of degree 2 model is much smoother, it predicts a parabola with some noise\n",
    "    - This seems a more reasonable model even though it has a lower $R^2$\n",
    "* If we collected more data points, say (10,9) and (7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([4.,5.,6., 7, 9.,10,12, 14.])\n",
    "y = np.array([4.2, 6., 6., 7, 9., 9, 10, 10.])\n",
    "plot_poly(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice how the degree 5 model changed shape and its $R^2$ went down.\n",
    "* Degree 1 and 2 models changed very little. They were better predictors of the new data\n",
    "* Models that overfit the data have high variance.\n",
    "\n",
    "#### Underfitting\n",
    "\n",
    "* Model with degree 0 has no coefficients so the model is just a model of the dependent variable alone. The orange line represents the mean of the dependent variable y. It is not capturing any patterns in the data.\n",
    "* Models that underfit the data, like the degree 0 and degree 1 models, have high bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "np.random.seed(42)\n",
    "N = 10 # 10 Polynomials, degree 1 - 10\n",
    "\n",
    "# Generate some noisy nonlinear data\n",
    "\n",
    "x = np.array([i*np.pi/180 for i in range(60,300,4)]) # radians 60-300 degrees\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x)) # Add some noise to sin function\n",
    "print(x.shape,y.shape)\n",
    "plt.plot(x,y,'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an array where rows are the inputs and the columns are polynomials $x^1$ to $x^{10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x**(n+1) for n in range(N)]).T\n",
    "# Rows of Nth degree polynomial for 60 data points on the x-axis\n",
    "print(X[0,:])\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N Polynomial Regression models\n",
    "\n",
    "* Accumulate model coefficients, predictions and mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression() for i in range(N)]\n",
    "coefs = np.zeros((N,N))\n",
    "yhats = []\n",
    "mses = np.zeros(N)\n",
    "scores = np.zeros(N)\n",
    "r2 = np.zeros(N)\n",
    "for i in range(N):\n",
    "    models[i].fit(X[:,0:(i+1)], y)\n",
    "    yhats.append(models[i].predict(X[:,0:(i+1)]))\n",
    "    for j in range(len(models[i].coef_)):\n",
    "        coefs[i,j] = models[i].coef_[j]\n",
    "    mses[i] = np.mean((y - yhats[-1])**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print coefficient matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Deg  Coefficients')\n",
    "for i in range(N):\n",
    "    a = [x for x in coefs[i]]\n",
    "    print(i+1, list(map(lambda x: round(x,3), a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(mses)),mses)\n",
    "plt.title(\"MSE\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data and regression line for models 1,4,7,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,((ax0,ax1),(ax2,ax3)) = plt.subplots(2,2,figsize = (12,6))\n",
    "axs = [ax0,ax1,ax2,ax3]\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.plot(x,y,'o')\n",
    "    ax.plot(x,yhats[i*3])\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # Using a different seed\n",
    "x = np.array([i*np.pi/180 for i in range(60,300,4)])\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x))\n",
    "X = np.array([x**(n+1) for n in range(N)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.zeros((N,N))\n",
    "yhats = []\n",
    "mses_test = np.zeros(N)\n",
    "for i in range(N):\n",
    "    models[i].fit(X[:,0:(i+1)], y)\n",
    "    yhats.append(models[i].predict(X[:,0:(i+1)]))\n",
    "    for j in range(len(models[i].coef_)):\n",
    "        coefs[i,j] = models[i].coef_[j]\n",
    "    mses_test[i] = np.mean((y - yhats[-1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plt.plot(range(len(mses)),mses)\n",
    "p2 = plt.plot(range(len(mses_test)),mses_test)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend((p1[0],p2[0]),(\"Training\",\"Test\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "* Higher MSE for new data\n",
    "* Magnitude of coefficients increased with model complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
