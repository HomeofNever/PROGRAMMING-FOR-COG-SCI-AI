{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports for Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV \n",
    "from sklearn.linear_model import Lasso,LassoCV\n",
    "from sklearn.linear_model import ElasticNet,ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "* Regularization methods are intended to prevent or reduce overfitting thus improving generalization (i.e. improve performance on test set)\n",
    "\n",
    "* The methods described here penalize the model by adding additional constraints to the parameter optimization:\n",
    "    - Ridge Regression\n",
    "    - Lasso Regression\n",
    "    - Elastic Net Regression\n",
    "\n",
    "* These regularization methods add a constraint(s) to the Ordinary Least Squares linear regression cost function that constrains (i.e. shrinks) the coefficients.\n",
    "    -  Called Shrinkage Methods\n",
    "\n",
    "#### Why favor smaller coefficients\n",
    "\n",
    "* The larger the magnitude of the coefficient that greater the change in the response (i.e. greater variability)\n",
    "* Do not want a lot of weight for any single predictor\n",
    "    - If different training data, response could vary with just slight change in this one predictor.\n",
    "        - Response would be sensitive overly sensitive to this predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "* In Oridinary Least Squares regression we estimate the coefficients $\\beta$:\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$ \\hat{\\beta} =   \\underset{\\beta}{\\mathrm{argmin}}\\sum_i{(y_i - X_i^T\\beta)^2}$$\n",
    "</div>\n",
    "\n",
    "* This produces the vector of coefficients $\\beta$ that is the **Best Linear Unbiased Estimator (BLUE)**\n",
    "    - Of all the unbiased estimators (that is the ones that $\\frac{1}{N}\\sum_1^\\infty{\\hat{\\beta}}$  = the population $\\beta$) it has the lowest variance\n",
    "        - This is a theorem: Gauss-Markov theorem\n",
    "    - But this may not have the best test set performance because the of the variance\n",
    "\n",
    "* Ridge Regression adds a penalty equivalent to square of the magnitude of coefficients ($\\beta^2$) to the OLS error\n",
    "    - L2 regularization\n",
    "    - This shrinks the coefficients toward zero but not to zero\n",
    "    \n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\hat{\\beta} =  \\underset{\\beta}{\\mathrm{argmin}}\\sum_i{(y_i - X_i^T\\beta)^2} + \\lambda \\sum_{j=1}^p\\beta_j^2$$\n",
    "</div>\n",
    "\n",
    "* By shrinking the coefficients, Ridge Regression constrains the model to fewer choices thus increasing the bias by a little, but the variance is greatly reduced due to the decrease in complexity\n",
    "\n",
    "#### $\\lambda$ Parameter\n",
    "\n",
    "* Controls amount of regularization\n",
    "\n",
    "* $\\lambda$ = 0: same as OLS Regression\n",
    "\n",
    "* $\\lambda$ >> 0: drives all the coefficients to very close to zero \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression in sklearn\n",
    "\n",
    "#### Ridge Class\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "* **The alpha keyword parameter is the $\\lambda$ hyperparameter**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Hitters data\n",
    "\n",
    "* Dependent variable: Salary\n",
    "* 16 numerical, 3 categorical predictors\n",
    "* This dataset has null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hitters.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop missing values and use only numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.drop(['League','Division','NewLeague'],axis = 1)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create arrays and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:16].values\n",
    "y = df.iloc[:,16].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size = 0.25, random_state = 1234)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients, $R^2$, and Mean Square Error of Ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r = Ridge(alpha = 10)\n",
    "model_r.fit(X_train,y_train)\n",
    "coefs_r = model_r.coef_\n",
    "print('R-Squared: ',model_r.score(X_train,y_train))\n",
    "yhat = model_r.predict(X_test)\n",
    "mse = np.mean((y_test - yhat)**2)\n",
    "print('MSE: ',mse)\n",
    "print(coefs_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients, $R^2$, and Mean Square Error of OLS regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l = LinearRegression()\n",
    "model_l.fit(X_train,y_train)\n",
    "coefs_l = model_l.coef_\n",
    "print('R-Squared: ',model_l.score(X_train,y_train))\n",
    "yhat = model_l.predict(X_test)\n",
    "mse = np.mean((y_test - yhat)**2)\n",
    "print('MSE: ',mse)\n",
    "print(coefs_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show shrinkage of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plt.plot(range(len(coefs_l)),coefs_l,'r') \n",
    "p2 = plt.plot(range(len(coefs_r)),coefs_r,'b') \n",
    "plt.title(\"Shrinkage\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.legend((p1[0],p2[0]),(\"Linear\",\"Ridge\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that no coefficients shrunk to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of coefficients ridge regression shrunk coefficients to 0: {np.sum(coefs_r == 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Cross Validation: RidgeCV Class\n",
    "\n",
    "* Cross Validation for alpha\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RidgeCV(store_cv_values = True)\n",
    "model.fit(X_train,y_train)\n",
    "coefs_r = model.coef_\n",
    "print('R-Squared: ',model.score(X_train,y_train))\n",
    "print('Alpha: ', model.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.cv_values_[0])\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "* Similar to Ridge Regression\n",
    "\n",
    "* It adds penalty equivalent to absolute value of the magnitude of coefficients to the OLS error\n",
    "    - L1 regularization\n",
    "\n",
    "* This can shrink the coefficients to zero \n",
    "\n",
    "* Like ridge Regression it increases bias a little but decreases variance by decreasing the number of predictors\n",
    "\n",
    "* A form of model selection because it eliminates predictors\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\hat{\\beta} =  \\underset{\\beta}{\\mathrm{argmin}}\\sum_i{(y_i - X_i^T\\beta)^2} + \\lambda \\sum_{j=1}^p|\\beta_j|$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression in sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients, $R^2$, and Mean Square Error of Ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha = 10.0)\n",
    "model.fit(X_train,y_train)\n",
    "coefs_lasso = model.coef_\n",
    "print('R-Squared: ',model.score(X_train,y_train))\n",
    "yhat = model.predict(X_test)\n",
    "mse = np.mean((y_test - yhat)**2)\n",
    "print('MSE: ',mse)\n",
    "plt.plot(range(len(coefs_lasso)),coefs_lasso,'b')   \n",
    "print(f'Number of coefficients lasso regression shrunk to 0: {np.sum(coefs_lasso == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,v in zip(df.columns,coefs_lasso):\n",
    "    if v != 0: print(n,round(v,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Cross Validation, sklearn LassoCV class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "model = LassoCV()\n",
    "model.fit(X,y)\n",
    "coefs_lasso = model.coef_\n",
    "print('R-Squared: ',model.score(X_train,y_train))\n",
    "plt.plot(range(len(coefs_lasso)),coefs_lasso,'b')   \n",
    "print(f'Number of coefficients lasso regression shrunk to 0: {np.sum(coefs_lasso == 0)}')\n",
    "print('Alpha: ', model.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,v in zip(df.columns,coefs_lasso):\n",
    "    if v != 0: print(n,round(v,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(df.loc[:,'Years'],df.loc[:,'Salary'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.loc[:,'Years'],df.loc[:,'Salary']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge versus Lasso\n",
    "\n",
    "#### When to use\n",
    "\n",
    "* In general you use the shrinkage methods when you have many predictors\n",
    "    - At least 3\n",
    "* When you know that each predictor contributes something then use Ridge Regression\n",
    "* When you suspect that some of the predictors are useless then use Lasso Regression\n",
    "\n",
    "#### Constraint formulation\n",
    "\n",
    "* Ridge\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\sum_i{(y_i - X_i^T\\beta)^2} \\text{ subject to } ||\\beta||_2^2 \\le c^2$$\n",
    "</div>\n",
    "* Lasso\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\sum_i{(y_i - X_i^T\\beta)^2} \\text{ subject to } ||\\beta||_1 \\le c$$\n",
    "</div>\n",
    "\n",
    "#### Constrained Optimization: Lagrange Multipliers\n",
    "* Ridge\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$ L(\\beta,\\lambda) =  \\sum_i{(y_i - X_i^T\\beta)^2} + \\lambda(||\\beta||_2^2 - c^2)$$\n",
    "$$ = ||y-X\\beta||_2^2 + \\lambda||\\beta||_2^2$$\n",
    "</div>\n",
    "* Lasso\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$ L(\\beta,\\lambda) =  \\sum_i{(y_i - X_i^T\\beta)^2} + \\lambda(||\\beta||_1 - c)$$\n",
    "$$ = ||y-X\\beta||_2^2 + \\lambda||\\beta||_2^2$$\n",
    "</div>\n",
    "\n",
    "#### Geometric Interpretation\n",
    "\n",
    "![](GeoIntrp.png)\n",
    "$$\\text{Figure 1. Left: Ridge, Right: Lasso}$$\n",
    "\n",
    "Figure source: http://www.astroml.org/book_figures/chapter8/fig_lasso_ridge.html\n",
    "\n",
    "#### Solution\n",
    "* OLS solution to unconstrained problem: $(X^TX)^{-1}X^Ty$\n",
    "\n",
    "* Ridge solution to constrained problem:\n",
    "$$(X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "* Lasso solution to constrained problem: No closed form solution, solve by numerical analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "\n",
    "* Combines Ridge and Lasso regression\n",
    "* This combination allows:\n",
    "    - For learning a sparse model where few of the weights are non-zero (like Lasso)\n",
    "    - While still maintaining the regularization properties of Ridge. \n",
    "* Use when have a lot of variables and you don't know how each contributes or even if they are useful.\n",
    "\n",
    "* Two $\\lambda$s, $\\lambda_1$ for Lasso and $\\lambda_2$ for Ridge\n",
    "\n",
    "<div style=\"font-size: 110%;\">\n",
    "$$\\hat{\\beta} =  \\underset{\\beta}{\\mathrm{argmin}}\\sum_i{(y_i - X_i^T\\beta)^2} + \\lambda_1 \\sum_{j=1}^p|\\beta_j| + \\lambda_2 \\sum_{j=1}^p\\beta_j^2$$\n",
    "</div>\n",
    "\n",
    "* Use Cross Validation to determine the best $\\lambda_1$ and $\\lambda_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Elastic Net Regression\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "\n",
    "* $\\alpha$: the penalty weighting (i.e. $\\lambda$)\n",
    "\n",
    "* l1_ratio: the ElasticNet mixing parameter. $ 0 \\le \\text{l1_ratio} \\le 1$\n",
    "    - It determines the scaling of $\\lambda_1$ and $\\lambda_2$\n",
    "    - l1_ratio = 0: the penalty is a L2 penalty\n",
    "    - l1_ratio = 1: the penalty is a L1 penalty\n",
    "    - For 0 < l1_ratio < 1 it is a combination of L1 and L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet(l1_ratio = 0.5)\n",
    "model.fit(X_train,y_train)\n",
    "coefs_EN = model.coef_\n",
    "print('R-Squared: ',model.score(X_train,y_train))\n",
    "yhat = model.predict(X_test)\n",
    "mse = np.mean((y_test - yhat)**2)\n",
    "print('MSE: ',mse)\n",
    "plt.plot(range(len(coefs_EN)),coefs_EN,'b')   \n",
    "print(f'Number of coefficients ElasticNet regression shrunk to 0: {np.sum(coefs_EN == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,v in zip(df.columns,coefs_EN):\n",
    "    if v != 0: print(n,round(v,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn Elastic Net Regression Cross Vaidation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html\n",
    "\n",
    "* l1_ratio:\n",
    "    - When a number:\n",
    "        * Weight for L1 constraint is alpha*l1_ratio and weight for L2 term is 0.5*alpha*(1 - l1_ratio)\n",
    "    - When a list, the different values are tested by cross-validation\n",
    "        - It is noted a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge)\n",
    "* $\\alpha$: the penalty weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "model = ElasticNetCV(l1_ratio = [.1, .5, .7, .9, .95, .99, 1])\n",
    "model.fit(X_train,y_train)\n",
    "coefs_EN = model.coef_\n",
    "plt.plot(range(len(coefs_EN)),coefs_EN,'b')   \n",
    "print(f'Number of coefficients ElasticNet regression shrunk to 0: {np.sum(coefs_EN == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,v in zip(df.columns,coefs_EN):\n",
    "    if v != 0: print(n,round(v,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv(\"College2.csv\",index_col = 0)\n",
    "college.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using college.csv, which contains:\n",
    "\n",
    "A data frame with 777 observations on the following 18 variables.  \n",
    "\n",
    "Private: No or Yes indicating private or public university  \n",
    "Apps: Number of applications received  \n",
    "Accept: Number of applications accepted  \n",
    "Enroll: Number of new students enrolled  \n",
    "Top10perc: Pct. new students from top 10% of H.S. class  \n",
    "Top25perc: Pct. new students from top 25% of H.S. class  \n",
    "F.Undergrad: Number of fulltime undergraduates  \n",
    "P.Undergrad: Number of parttime undergraduates  \n",
    "Outstate: Out-of-state tuition  \n",
    "Room.Board: Room and board costs  \n",
    "Books: Estimated book costs  \n",
    "Personal: Estimated personal spending  \n",
    "PhD: Pct. of faculty with Ph.D.'s  \n",
    "Terminal: Pct. of faculty with terminal degree  \n",
    "S.F.Ratio: Student/faculty ratio  \n",
    "perc.alumni: Pct. alumni who donate  \n",
    "Expend: Instructional expenditure per student  \n",
    "Grad.Rate: Graduation rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(len(college.columns)))\n",
    "idx.remove(1)\n",
    "X = college.iloc[:, idx].values\n",
    "y = college.iloc[:, 1].values # number of applications\n",
    "\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.loc['Rensselaer Polytechnic Institute']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ridge Regression\n",
    "\n",
    "Predict Apps ( Number of applications received) with a Ridge regression model using RidgeCV.  \n",
    "Output $R^2$ and MSE for the test data and the best alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso Regression\n",
    "\n",
    "Predict Apps ( Number of applications received) with a Lasso regression model using LassoCV. Output $R^2$ and MSE for the test data and the best alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recursive Feature Elimination\n",
    "\n",
    "Use RFECV on a Lasso model instance to determine the features to eliminate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use RFECV fit.support\n",
    "\n",
    "Predict Apps with a Linear regression model using the variables selected by the RFECV object. Output $R^2$ and MSE for the test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Elastic Net Regression\n",
    "\n",
    "Fit an ElasticNet Cross-Validated model to the training data.  \n",
    "Plot the coefficients.  \n",
    "Print the number of coefficients shrunk to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "181px",
    "left": "1047px",
    "right": "20px",
    "top": "120px",
    "width": "325px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
