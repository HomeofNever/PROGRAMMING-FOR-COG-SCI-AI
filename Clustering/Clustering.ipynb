{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs,make_moons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "import scipy.cluster.hierarchy as clusterHierarchy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram,linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Clustering  \n",
    "\n",
    "* Unsupervised learning method (i.e. no labels)\n",
    "    - Data: $(x_1,x_2,...,x_n), \\text{  }x \\in R^p$\n",
    "* Broad set of techniques for finding subgroups or clusters in the data\n",
    "* Partition data into groups so that observations within a group are **similar** to each other\n",
    "* Meaning of similar can depend on the domain\n",
    "* Comparison to PCA\n",
    "    - PCA looks for a low-dimensional representation of the observations that explains a good fraction of the variance.\n",
    "    - Clustering looks for **homogeneous subgroups** among the observations.\n",
    "* Clustering methods\n",
    "    - Prototype-based Clustering\n",
    "        - Centroid: average of similar points with continuous features\n",
    "        - Medoid: most representative or most frequently occurring point for categorical features\n",
    "        - Modeler specifies number of clusters\n",
    "    - Hierarchical\n",
    "        - Numbers of clusters not known\n",
    "    - Density-based clustering\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           \n",
    "### K-Means clustering\n",
    "\n",
    "* A prototype-based algorithm\n",
    "* Easy to implement and computationally efficient compared to other clustering algorithms\n",
    "* Observations in the same cluster will be 'similar' based on a distance metric\n",
    "* The modeler must specify k, the number of clusters\n",
    "* Assumes at least one item per cluster and that clusters do not overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize = (10,6))\n",
    "sns.scatterplot(x='petal_length',y='petal_width',data=iris,ax = ax1)\n",
    "sns.scatterplot(x='sepal_length',y='sepal_width',data=iris,ax = ax2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize = (10,6))\n",
    "sns.scatterplot(x='petal_length',y='petal_width',hue='species',data=iris, ax = ax1)\n",
    "sns.scatterplot(x='sepal_length',y='sepal_width',hue='species',data=iris, ax =ax2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Measure\n",
    "\n",
    "* For continuous features: squared Euclidean distance between x and y, $x,y \\in R^m$\n",
    "\n",
    "<div style=\"font-size: 115%;\">\n",
    "$$ d(\\vec{x},\\vec{y})^2 = \\sum_{j=1}^{m}(x_j - y_j)^2 = ||\\vec{x} - \\vec{y}||_2^2$$\n",
    "</div>\n",
    "\n",
    "* The index j refers to the j-dimension (i.e. jth feature) of the $(\\vec{x},\\vec{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means as optimization\n",
    "\n",
    "Let:  \n",
    "i = sample index  \n",
    "j = cluster index  \n",
    "k = number of clusters  \n",
    "n = number of samples\n",
    "  \n",
    "* Each observation $x^{(i)}$ belongs to one cluster j  \n",
    "\n",
    "* Clusters are non-overlapping  \n",
    "\n",
    "#### Cluster Inertia\n",
    "\n",
    "* Cluster inertia: The within-cluster Sum of Squared Errors (SSE)  \n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$SSE = \\sum_{i=1}^{n}\\sum_{j=1}^{k}w^{(i,j)}||x^{(i)} - \\mu^{(j)}||_2^2$$\n",
    "</div>\n",
    "\n",
    "$\\mu^{(j)}$ is the centroid for cluster j     \n",
    "$w^{(i,j)}$ = 1 if sample $x^{(i)}$ is in th cluster j and 0 otherwise  \n",
    "\n",
    "* Inertia is a measure of how internally coherent clusters are.\n",
    "\n",
    "* The K-Means algorithm is an iterative procedure for minimizing the within cluster SSE.\n",
    "\n",
    "* Choice of starting points can change cluster outcome\n",
    "    - Run algorithm many times with different randomly chosen starting points  \n",
    "\n",
    "* Drawbacks:\n",
    "    - Inertia makes the assumption that clusters are convex \n",
    "    - It responds poorly to elongated clusters, or irregular shapes.\n",
    "    - Inertia only tells us that lower values are better and zero is optimal. (i.e. not normalized)\n",
    "        - In very high-dimensional spaces, Euclidean distances tend to become inflated (curse of dimensionality). \n",
    "        - Use PCA prior to k-means clustering  to alleviate this problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means Clustering Algorithm\n",
    " \n",
    "1. Randomly pick k centroids from the sample points as initial cluster centers    \n",
    "2. Assign each sample to the nearest centroid $\\mu^{(j)}$, j $\\in${1,...,k}  \n",
    "3. Calculate the new centroids based on the assignments made in step 2. \n",
    "4. Repeat steps 2 and 3 until the cluster assignments do not change or a maximum number of iterations is reached.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn K-Means\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "* Since calculating distances, when using real data need to scale the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans parameters\n",
    " \n",
    "* n_clusters: Number of clusters\n",
    "* n_init: number of times to run with different centroid seeds\n",
    "* init: Initialization method\n",
    "* max_iter: Maximum number of iterations for a single run.\n",
    "    - KMeans will stop before max_iter if it converges (i.e. within-cluster SSE doesn't change within tolerance)\n",
    "* tol:  controls the tolerance to changes in the within-cluster SSE to declare convergence.\n",
    "    - Larger values will allow faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=150,\n",
    "                    n_features=2,\n",
    "                      centers=3,\n",
    "                      cluster_std=0.75,\n",
    "                      shuffle=True,\n",
    "                      random_state=0)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y, marker='o', edgecolor='black', s=50)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:11,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  KMeans(n_clusters=3,\n",
    "                init='random',\n",
    "                n_init=10,  # Run 10 times with with different centroid seeds\n",
    "                max_iter=300,\n",
    "                tol=1e-04,\n",
    "                random_state=42)\n",
    "\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict new sample (-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(np.array([-1,4]).reshape(-1,2))\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  KMeans output\n",
    " \n",
    "* cluster_centers: coordinates of cluster centers\n",
    "* labels_: Assigned label for each point\n",
    "* inertia_: Sum of squared distances of samples to their closest cluster center.  \n",
    "* n_iter: Number of iterations run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cluster Centers \\n {model.cluster_centers_}')\n",
    "print(f'Model Labels \\n {model.labels_}')\n",
    "print(\"Model Inertia: \", model.inertia_)\n",
    "print(f'Predicted Cluster {yhat[0]} for new point (-1,4)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X1': X[:,0],'X2': X[:,1], 'y':model.labels_})\n",
    "\n",
    "def plot_kmeans(model,df,yhat):\n",
    "    sns.scatterplot(x = 'X1', y = 'X2',hue = 'y',data = df)\n",
    "    plt.plot(model.cluster_centers_[:,0],model.cluster_centers_[:,1], 'ro', marker = '*',markersize=10)\n",
    "    plt.plot(-1,4, 'bo', marker = 'X',markersize=10)\n",
    "    plt.title('Stars: Cluster Centers, Cross: Predicted sample')\n",
    "    \n",
    "plot_kmeans(model,df,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues with KMeans\n",
    "\n",
    "* Random centroid initialization can produce very different results\n",
    "* Modeler must choose number of clusters\n",
    "* Makes assumptions about shape of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means++\n",
    "\n",
    "* An improved way to place initial cluster centroids\n",
    "\n",
    "* In regular K-Means centers are chosen randomly and run multiple times and choose best performer \n",
    "    - Can result in bad centers or slow convergence\n",
    "  \n",
    "* To improve upon this Arthur and Vassilvitskii (2007) developed the K-Means++ centroid initialization algorithm\n",
    "    - Centers should be as far from each other as possible\n",
    "\n",
    "#### K-Means++ init algorithm\n",
    "\n",
    "* Place centroids far from each other according the the following algorithm\n",
    "\n",
    "From the original paper: http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf  \n",
    "\n",
    "Let X be array of sample observations.  \n",
    "\n",
    "Let D(x) denote the shortest distance from a data point to the closest center we have already chosen\n",
    "\n",
    "1a. Take one center $c_i$, chosen uniformly at random from X  \n",
    "1b. Take a new center $c_i$, choosing x $\\in$  X with probability \n",
    "\n",
    "$$P(c_i) = \\frac{D(x)^2}{\\sum_{x \\in X}D(x)^2}$$\n",
    "\n",
    "1c. Repeat Step 1b. until we have taken k centers altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kmeans(X, k):\n",
    "    idx = np.random.randint(0,len(X))\n",
    "    C = [X[idx]]\n",
    "    for k in range(1, k):\n",
    "        print('C: ', C)\n",
    "        D2 = np.array([min([np.dot(c-x,c-x) for c in C]) for x in X])\n",
    "        probs = D2/D2.sum()  # P(c_i)\n",
    "        print('Distance: ',D2)\n",
    "        print('Probs: ', probs)\n",
    "        C.append(np.random.choice(X,size=1,p=probs)[0]) # Choosing from X according to P(c_i)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = [0,1,2,3,4]\n",
    "k = 3\n",
    "init_kmeans(x,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means++ in sklearn\n",
    "\n",
    "\n",
    "* init paramater = 'k-means++'\n",
    "    - this is the default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  KMeans(n_clusters=3, n_init=10, max_iter=300, tol=1e-04, random_state=42)\n",
    "model.fit(X)\n",
    "yhat = model.predict(np.array([-1,4]).reshape(-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cluster Centers \\n {model.cluster_centers_}')\n",
    "print(f'Model Labels \\n {model.labels_}')\n",
    "print(\"Model Inertia: \", model.inertia_)\n",
    "print(f'Predicted Cluster {yhat[0]} for new point (-1,4)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X1': X[:,0],'X2': X[:,1], 'y':model.labels_})\n",
    "plot_kmeans(model,df,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method to find best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elbow(X):\n",
    "    SSEs = []\n",
    "    for i in range(1, 11):\n",
    "        model = KMeans(n_clusters=i,\n",
    "                init='k-means++',\n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=42)\n",
    "        model.fit(X)\n",
    "        SSEs.append(model.inertia_)\n",
    "\n",
    "    plt.plot(range(1,11), SSEs, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('SSEs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elbow(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=150,\n",
    "                    n_features=2,\n",
    "                      centers=5,\n",
    "                      cluster_std= 1.5,\n",
    "                      shuffle=True,\n",
    "                      random_state=0)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1],c=y,marker='o', edgecolor='black',s=50)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elbow(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Analysis\n",
    "\n",
    "* Quantify the quality of the clustering performed\n",
    "\n",
    "* **silhouette coefficient**: a measure how tightly grouped the samples in the cluster are.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "1. Calculate **cluster cohesion**: $a^{(i)}$ as the average distance between a sample $x^{(i)}$ and all other samples in the same cluster\n",
    "\n",
    "2. Calculate **cluster separation**: $b^{(i)}$ from the next closest cluster for a sample $x^{(i)}$ as the average distance between the sample and all samples in that cluster.\n",
    "\n",
    "3. Calculate the silhouette $s^{(i)}$ as the difference between cluster cohesion and separation divided by the greater of the two.\n",
    "\n",
    "<div style=\"font-size: 125%;\">\n",
    "$$ s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{max(b^{(i)},a^{(i)})}$$\n",
    "</div>\n",
    "\n",
    "* $-1 \\le s^{(i)} \\le 1$\n",
    "\n",
    "* $b^{(i)}$: quantifies how dissimilar a sample is to other clusters\n",
    "\n",
    "* $a^{(i)}$: quantifies within cluster similarity\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "* The best value is 1 and the worst value is -1. \n",
    "* Values near 0 indicate overlapping clusters. \n",
    "* Negative values generally indicate that a sample has been assigned to the wrong cluster (i.e.a different cluster is more similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn silhouette_samples\n",
    "\n",
    "* Calculates silhouette coefficient for each data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model =  KMeans(n_clusters=5, n_init=10, max_iter=300, tol=1e-04, random_state=42)\n",
    "model.fit(X)\n",
    "yhat = model.predict(X)\n",
    "silhouette_vals = silhouette_samples(X,yhat,metric='euclidean')\n",
    "print(silhouette_vals.shape)\n",
    "sil_mean = np.mean(silhouette_vals)\n",
    "print(f'Silhouette mean: {np.round(sil_mean,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_vals = []\n",
    "for i,v in enumerate(silhouette_vals):\n",
    "    if v < 0: bad_vals.append((i,v)) \n",
    "bad_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMEANS with Mall Customers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cust = pd.read_csv('Mall_Customers.csv')\n",
    "Cust.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Income',y='Spending',data=Cust);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Cust.iloc[:, [3, 4]].values\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_s = sc.fit_transform(X)\n",
    "\n",
    "Elbow(X_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "# Fit to the data\n",
    "model = KMeans(n_clusters = k, n_init = 20, random_state = 42)\n",
    "clusters = model.fit_predict(X_s)\n",
    "print('Model inertia: ',model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cust['Labels'] = model.labels_\n",
    "Cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "centers  = sc.inverse_transform(model.cluster_centers_)\n",
    "fig,ax = plt.subplots(figsize = (12,6))\n",
    "sns.scatterplot(x = 'Income', y = 'Spending',hue = 'Labels',data = Cust,ax = ax)\n",
    "ax.scatter(centers[:, 0], centers[:, 1], s = 100, c = 'red', label = 'Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Spending')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = silhouette_samples(X_s,clusters,metric='euclidean')\n",
    "print(f'Silhouette coefficient {np.round(np.mean(vals),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Hierarchical Clustering\n",
    "\n",
    "* Don't choose K ahead of time\n",
    "* Bottom-up or agglomerative clustering\n",
    "    - Build dendrogram from leaves to trunk\n",
    "* Requires manual cutoff point\n",
    "     \n",
    "#### Hierarchical Clustering Algorithm:\n",
    "\n",
    "Given a set of N items to be clustered and an NxN distance (or similarity) matrix  \n",
    "1.  Start by assigning each observation to its own cluster. You have N clusters. The distances (or dissimilarities)   between clusters is just the distances (or dissimilarities) between the items.  \n",
    "2.  Find the closest (most similar) pair of clusters and merge them into a single cluster.  \n",
    "3.  Compute distances (dissimilarities) between the new cluster and each of the old clusters  \n",
    "4.  Repeat steps 2 and 3 until all items are clustered into a single cluster of size N  \n",
    "5. Step 3, comparing two groups, can be done in different ways called linkage  \n",
    "    \n",
    "#### Linkage - how to compare two clusters\n",
    " \n",
    "* Complete: Maximal intercluster dissimilarity.Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities. (i.e. the ones farthest apart)\n",
    "* Single: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. (i.e. the closest ones)\n",
    "* Average: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.\n",
    "* Centroid - The distance between the centroids of the clusters\n",
    "    \n",
    "#### Choice of Dissimilarity Measure\n",
    " \n",
    "* Euclidean distance\n",
    "* Correlation\n",
    "    \n",
    "#### Plot using Dendrogram\n",
    "\n",
    "* Length of vertical bars is the distance measure\n",
    "* Where to cut?\n",
    "    - Heuristic: Find longest vertical bar that doesn't intersect with a horizontal line (i.e. if you extend the horizontal lines)\n",
    "    \n",
    "![](dendrogram.png)\n",
    "$$\\text{Figure 1. Clustering with Dendrogram}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering with Complete Linkage\n",
    "\n",
    "1. Compute the distance matrix of all samples.\n",
    "2. Represent each data point as a singleton cluster.\n",
    "3. Merge the two closest clusters based on the distance between the most dissimilar (distant) members.\n",
    "4. Update the similarity matrix.\n",
    "5. Repeat steps 2-4 until one single cluster remains.\n",
    "\n",
    "#### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "variables = ['X', 'Y', 'Z']\n",
    "labels = ['A','E','D','B','C']\n",
    "X = np.random.random_sample([5,3])*10\n",
    "df = pd.DataFrame(X, columns=variables, index=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy distance metric\n",
    "\n",
    "* calculates specified distance metric between pairwise samples in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = pdist(df, metric='euclidean')\n",
    "d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy squareform\n",
    "\n",
    "* converts scipy pdist output to a symmetric square matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = squareform(d)\n",
    "row_dist = pd.DataFrame(s,columns=labels, index=labels)\n",
    "row_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply complete linkage agglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_clusters = linkage(d,method='complete') # from pdist\n",
    "row_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_clusters = linkage(df.values,method='complete') # data frame input\n",
    "row_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(row_clusters,\n",
    "    columns=['row label 1','row label 2','distance','no. of items in clust.'],\n",
    "    index=['cluster %d' %(i+1) for i in range(row_clusters.shape[0])])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_dendr = dendrogram(row_clusters,labels=labels)\n",
    "plt.ylabel('Euclidean distance')\n",
    "plt.tight_layout()\n",
    "plt.title(\"Dendrogram\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering in sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering\n",
    "Cust = pd.read_csv('Mall_Customers.csv')\n",
    "X = Cust.iloc[:, [3, 4]].values\n",
    "\n",
    "# Use the dendrogram to find the optimal number of clusters\n",
    "model = clusterHierarchy.linkage(X, method = 'complete',metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,12))\n",
    "dgram = clusterHierarchy.dendrogram(model)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 5\n",
    "# Fit to the data\n",
    "m = AgglomerativeClustering(n_clusters = cut, affinity = 'euclidean', linkage = 'complete')\n",
    "clusters = m.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cust['Clusters'] = clusters\n",
    "\n",
    "# Visualize the clusters\n",
    "fig,ax = plt.subplots(figsize = (12,6))\n",
    "sns.scatterplot(x = 'Income', y = 'Spending',hue = 'Clusters',data = Cust,ax = ax)\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Spending')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "* Assigns cluster labels based on dense regions of points\n",
    "* Doesn't make assumption about spherical clusters (e.g. k-means)\n",
    "* Doesn't partition the dataset into hierarchies that require a manual cut-off point\n",
    "* **Density**: the number of points within a specified radius $\\epsilon$\n",
    "\n",
    "#### Labeling\n",
    "\n",
    "* Assign to each sample (point) using the following criteria:  \n",
    "    - A point is a **core point** if at least MinPts (e.g. 3) of neighboring points fall within the specified radius $\\epsilon$  \n",
    "    - A **border point** is a point that has fewer neighbors than MinPts within $\\epsilon$ , but lies within the $\\epsilon$ radius of a core point  \n",
    "    - All other points that are neither core nor border points are considered\n",
    "**noise points**\n",
    "\n",
    "![](DBSCAN.png)\n",
    "$$\\text{Figure 2. Density Based Clustering}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Algorithm\n",
    "\n",
    "1. Label the points\n",
    "2. Form a separate cluster for each core point or connected group of core points (i.e. core points are connected if they are no farther away than $\\epsilon$  ).\n",
    "3. Assign each border point to the cluster of its corresponding core point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare KMeans,  Hierarchical and DBScan Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
    "plt.scatter(X[:,0], X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans and Hierarchical (Agglomerative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2,random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "ac = AgglomerativeClustering(n_clusters=2,affinity='euclidean',linkage='complete')\n",
    "y_ac = ac.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "ax1.scatter(X[y_km==0,0],X[y_km==0,1],c='lightblue',edgecolor='black',marker='o',s=40, label='cluster 1')\n",
    "ax1.scatter(X[y_km==1,0],X[y_km==1,1],c='red',edgecolor='black',marker='s',s=40,label='cluster 2')\n",
    "ax1.set_title('K-means clustering')\n",
    "\n",
    "ax2.scatter(X[y_ac==0,0], X[y_ac==0,1],c='lightblue',edgecolor='black',marker='o',s=40,label='cluster 1')\n",
    "ax2.scatter(X[y_ac==1,0],X[y_ac==1,1],c='red',edgecolor='black',marker='s',s=40,label='cluster 2')\n",
    "ax2.set_title('Agglomerative clustering')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.2,min_samples=3,metric='euclidean')\n",
    "y_db = db.fit_predict(X)\n",
    "plt.scatter(X[y_db==0,0],X[y_db==0,1],c='lightblue',edgecolor='black',marker='o',s=40,label='cluster 1')\n",
    "plt.scatter(X[y_db==1,0],X[y_db==1,1],c='red',edgecolor='black',marker='s',s=40,label='cluster 2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "Raschka,Sebastin & Mirjalili, Vahid (2017). Python Machine Learning, 2nd Edition, Packt Publishing.\n",
    "\n",
    "Source Figure 1: \"An Introduction to Statistical Learning, with applications in R\"  (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
