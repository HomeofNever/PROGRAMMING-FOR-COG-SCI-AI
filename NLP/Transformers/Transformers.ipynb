{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "* State-of-the-art for processing sequences.\n",
    "* First successes were with text processing and machine translation in particular\n",
    "* Unlike RNNs, LSTMs and GRUs there are no recurrent connections so there is no memory of the previous state.\n",
    "* **Attention** overcomes the lack of memory by perceiving the entire sequence at once, enabling long-range semantic dependencies\n",
    "* Since there is no sequential processing, transformers are suited to parallel hardware environments \n",
    "\n",
    "* The transformer model follows the same general pattern as a standard sequence to sequence with attention model.\n",
    "\n",
    "    - The input sentence is passed through N encoder layers that generates an output for each word/token in the sequence.\n",
    "     - The decoder attends on the encoder's output and its own input (self-attention) to predict the next word.\n",
    "\n",
    "\n",
    "![](Transformer.png)\n",
    "$$\\text{Figure 1. Basic Transformer Architecture, Left: Encoder, Right: Decoder}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Basic Components for Machine Translation\n",
    "\n",
    "#### Input Encoder\n",
    "\n",
    "* Input Embedding \n",
    "    - The embedding for the source sentence (from the source language)\n",
    "* Positional Encoding \n",
    "    - Serial positions of the words in the source sentence \n",
    "* Multi-Head Attention \n",
    "    - Attention over the source sentence\n",
    "* Feed-Forward Network \n",
    "    - Computes non-linear hierarchical features\n",
    "    - Output goes to Multi-head attention in the decoder\n",
    "\n",
    "#### Output Decoder\n",
    "\n",
    "* Embedding \n",
    "    - The embedding for the target sentence (from the target language) \n",
    "* Positional Encoding \n",
    "    - Serial positions of the words generated so far\n",
    "* Masked Multi-Head Attention \n",
    "    - Attention over the words generated so far\n",
    "* Multi-Head Attention from Input and Output \n",
    "    - Query from the output, keys and values from the input\n",
    "* Feed-Forward Network \n",
    "    - Computes non-linear hierarchical features\n",
    "* Final Linear Layer with Softmax \n",
    "    - Probabilities for next word\n",
    "\n",
    "#### $N_x$\n",
    "\n",
    "* Number of endoder or decoder blocks to stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "* Ashish Vaswami, et.al. (2017) Attention Is All You Need\n",
    "\n",
    "* A way to capture dependencies among in sequences without sequential recurrent connections\n",
    "* Entire sequence processed in one step. Eliminates back-propagation through time\n",
    "\n",
    "#### Basic Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Attention(Q,K,V) = Softmax_k\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BasicAttention.png)\n",
    "$$\\text{Figure2. Basic Attention}$$\n",
    "\n",
    "* As a word is being processed we want information from all the other words in the sequence\n",
    "\n",
    "* Key-value store paradigm: Dictionary in Python\n",
    "    - Q Weight matrix representing a Query (i.e. What information in the rest of the sequence do I want for this word)\n",
    "    - K Weight matrix representing a Key (i.e. something like a label, e.g. color)\n",
    "    - V Weight matrix representing a Value, (i.e. the value of the label e.g. red)\n",
    "    \n",
    "* Think of these matrices as rotating the input vector in the embedding vector space\n",
    "* The key and the query must be the same dimension\n",
    "\n",
    "* The output of attention is a weigthed (i.e. probability) Value vector. (How should attention be allocated in the sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "![](Attention2.png)\n",
    "$$\\text{Figure 3. Multi-Head Attention}$$\n",
    "\n",
    "* Multiple attention (i.e. multiple hidden layers in Figure 3.) block in parallel to attend to different attributes. (e.g. color, gender, location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "* Attention is a parallel operation not sequential so word position is lost.\n",
    "* Positional Encoding add word position information\n",
    "* Different sine and cosine frequencies indicate relative position of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add & Norm\n",
    "\n",
    "* Add are residual connections to prevent vanishing gradients\n",
    "* Norm: Batch Normalization to stabilize the computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Systems Based on Attention-Transformer Architecture\n",
    "\n",
    "#### BERT - Bidirectional Encoder Representation from Transformers\n",
    "\n",
    "Google AI Language: https://arxiv.org/pdf/1810.04805.pdf (2019)\n",
    "\n",
    "* Stacked Encoders\n",
    "* Attention over left and right context (i.e. Bidirectional)\n",
    "* Training\n",
    "    - Pre-trained\n",
    "        - Masked Language Model: fill in words that are masked out\n",
    "        - Next Sentence Prediction: does second sentence follow from first\n",
    "    - Fine Tuning: must fine tune for each new language task\n",
    "* 110 million parameters\n",
    "\n",
    "#### GPT-2, GPT-3 : Generative Pre-Training\n",
    "\n",
    "Open AI  \n",
    "GPT-2:  https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf (2019)  \n",
    "GPT-3: https://arxiv.org/pdf/2005.14165.pdf (2020)  \n",
    "\n",
    "* Stacked Transformer Decoders\n",
    "* One-shot learning to do all language tasks.\n",
    "    - Don't need fine tuning for separate language tasks.\n",
    "* GPT-2 ~1.5 Billion Parameters, GPT-3 175 Billion Parameters  \n",
    "* GPT-3 is same architecture as GPT-2 only much much bigger  \n",
    "* GPT-3 trained with Common Crawl (i.e. the entire Internet)  \n",
    "\n",
    "GPT-2 online demo https://gpt2.ai-demo.xyz/\n",
    "\n",
    "* GPT-3 is much better than GPT-2 because the scale (training corpus and number of parameters) is so much bigger.\n",
    "\n",
    "* A lot of hype about GPT-3!!!\n",
    "\n",
    "* It appears to generate some realistic text but also can generate silly things\n",
    "\n",
    "https://machinelearningknowledge.ai/openai-gpt-3-demos-to-convince-you-that-ai-threat-is-real-or-is-it/\n",
    "\n",
    "### The Problem with Language Models\n",
    "\n",
    "**No Real Language Understanding**  \n",
    "**Language Understanding requires experience not just processing correlations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Ashish Vaswami, et.al. (2017) Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "https://www.kdnuggets.com/2020/08/transformer-architecture-development-transformer-models.html\n",
    "\n",
    "https://blog.exxactcorp.com/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
